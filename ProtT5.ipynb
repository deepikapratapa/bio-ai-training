{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e7cc799-e52c-4be6-b688-c2855f9f96cc",
   "metadata": {},
   "source": [
    "# ðŸ§¬ A ProtT5 Expedition with Dr. Aris\n",
    "\n",
    "Welcome back, researcher.\n",
    "\n",
    "In our earlier journeys, we:\n",
    "- **Listened to ESM-2** (a protein-native transformer trained at scale),\n",
    "- **Consulted ProtBERT** (a BERT-style reader of amino-acid â€œtextâ€).\n",
    "\n",
    "Today, Dr. Aris invites a new scholar into the lab:\n",
    "\n",
    "âœ… **ProtT5** â€” a T5-family protein language model, known for learning rich representations that often transfer well to downstream biology tasks.\n",
    "\n",
    "Our goal remains the same:\n",
    "**Enzyme vs Non-Enzyme classification** using **ProtT5 embeddings** + lightweight classifiers (LR / SVM / MLP)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c283422f-a4f9-4ce9-ae89-7f07b31652b7",
   "metadata": {},
   "source": [
    "### âš™ï¸ Environment Setup\n",
    "\n",
    "Before we open ProtT5â€™s manuscripts, we prepare the lab:\n",
    "- import libraries once\n",
    "- set random seeds\n",
    "- confirm whether we are running on GPU (`cuda`) or CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9042007b-8533-459b-bc2b-19ca49bbbe06",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on: cuda\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# Global Imports (Single Location)\n",
    "# ================================\n",
    "\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import time\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel, T5EncoderModel\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    average_precision_score,\n",
    "    f1_score,\n",
    "    accuracy_score\n",
    ")\n",
    "\n",
    "# ----------------\n",
    "# Environment setup\n",
    "# ----------------\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Running on:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e206df-86c2-489d-bbbd-25de447aa586",
   "metadata": {},
   "source": [
    "## ðŸ“‚ Returning to the Library of Proteins\n",
    "\n",
    "Dr. Aris retrieves the same curated collection we used before:\n",
    "- load UniProt CSV\n",
    "- clean sequences\n",
    "- (optional) balance to a fixed benchmark size\n",
    "- stratified train/test split\n",
    "\n",
    "This keeps the comparison fair across ESM-2, ProtBERT, and now ProtT5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a64b07c-cacf-4269-a254-3fff0acf8389",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/local/25423975/ipykernel_218052/2353240010.py:6: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(DATA_PATH)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: uniprot_swissprot_enzyme_nonenzyme_FULL.csv  shape: (537346, 9)\n",
      "De-duplicated by Entry: 537346 -> 537346\n",
      "\n",
      "After cleaning: (536963, 9)\n",
      "Label counts:\n",
      " label\n",
      "0    270121\n",
      "1    266842\n",
      "Name: count, dtype: int64\n",
      "\n",
      "After balancing: (40000, 9)\n",
      "Label counts:\n",
      " label\n",
      "0    20000\n",
      "1    20000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Train: (32000, 9) Test: (8000, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entry</th>\n",
       "      <th>Entry Name</th>\n",
       "      <th>Protein names</th>\n",
       "      <th>Organism</th>\n",
       "      <th>Length</th>\n",
       "      <th>EC number</th>\n",
       "      <th>Sequence</th>\n",
       "      <th>Reviewed</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17005</th>\n",
       "      <td>P74737</td>\n",
       "      <td>RECA_SYNY3</td>\n",
       "      <td>Protein RecA (Recombinase A)</td>\n",
       "      <td>Synechocystis sp. (strain ATCC 27184 / PCC 680...</td>\n",
       "      <td>354</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MASTNISDREKALNAALAQIERSFGKGAIMRLGDATQMRVETISTG...</td>\n",
       "      <td>reviewed</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19930</th>\n",
       "      <td>A1WXM7</td>\n",
       "      <td>SAHH_HALHL</td>\n",
       "      <td>Adenosylhomocysteinase (EC 3.13.2.1) (S-adenos...</td>\n",
       "      <td>Halorhodospira halophila (strain DSM 244 / SL1...</td>\n",
       "      <td>430</td>\n",
       "      <td>3.13.2.1</td>\n",
       "      <td>MSNQDYKVADISLADWGRKEIKIAESEMPGLMETRREFAAQKPLKG...</td>\n",
       "      <td>reviewed</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8641</th>\n",
       "      <td>B7ZR65</td>\n",
       "      <td>SOX9A_XENLA</td>\n",
       "      <td>Transcription factor Sox-9-A</td>\n",
       "      <td>Xenopus laevis (African clawed frog)</td>\n",
       "      <td>477</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MNLLDPFMKMTEEQDKCMSGAPSPTMSDDSAGSPCPSGSGSDTENT...</td>\n",
       "      <td>reviewed</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35026</th>\n",
       "      <td>P07435</td>\n",
       "      <td>OBP_BOVIN</td>\n",
       "      <td>Odorant-binding protein (OBP) (Olfactory mucos...</td>\n",
       "      <td>Bos taurus (Bovine)</td>\n",
       "      <td>159</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AQEEEAEQNLSELSGPWRTVYIGSTNPEKIQENGPFRTYFRELVFD...</td>\n",
       "      <td>reviewed</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19704</th>\n",
       "      <td>P53690</td>\n",
       "      <td>MMP14_MOUSE</td>\n",
       "      <td>Matrix metalloproteinase-14 (MMP-14) (EC 3.4.2...</td>\n",
       "      <td>Mus musculus (Mouse)</td>\n",
       "      <td>582</td>\n",
       "      <td>3.4.24.80</td>\n",
       "      <td>MSPAPRPSRSLLLPLLTLGTALASLGWAQGSNFSPEAWLQQYGYLP...</td>\n",
       "      <td>reviewed</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Entry   Entry Name                                      Protein names  \\\n",
       "17005  P74737   RECA_SYNY3                       Protein RecA (Recombinase A)   \n",
       "19930  A1WXM7   SAHH_HALHL  Adenosylhomocysteinase (EC 3.13.2.1) (S-adenos...   \n",
       "8641   B7ZR65  SOX9A_XENLA                       Transcription factor Sox-9-A   \n",
       "35026  P07435    OBP_BOVIN  Odorant-binding protein (OBP) (Olfactory mucos...   \n",
       "19704  P53690  MMP14_MOUSE  Matrix metalloproteinase-14 (MMP-14) (EC 3.4.2...   \n",
       "\n",
       "                                                Organism  Length  EC number  \\\n",
       "17005  Synechocystis sp. (strain ATCC 27184 / PCC 680...     354        NaN   \n",
       "19930  Halorhodospira halophila (strain DSM 244 / SL1...     430   3.13.2.1   \n",
       "8641                Xenopus laevis (African clawed frog)     477        NaN   \n",
       "35026                                Bos taurus (Bovine)     159        NaN   \n",
       "19704                               Mus musculus (Mouse)     582  3.4.24.80   \n",
       "\n",
       "                                                Sequence  Reviewed  label  \n",
       "17005  MASTNISDREKALNAALAQIERSFGKGAIMRLGDATQMRVETISTG...  reviewed      0  \n",
       "19930  MSNQDYKVADISLADWGRKEIKIAESEMPGLMETRREFAAQKPLKG...  reviewed      1  \n",
       "8641   MNLLDPFMKMTEEQDKCMSGAPSPTMSDDSAGSPCPSGSGSDTENT...  reviewed      0  \n",
       "35026  AQEEEAEQNLSELSGPWRTVYIGSTNPEKIQENGPFRTYFRELVFD...  reviewed      0  \n",
       "19704  MSPAPRPSRSLLLPLLTLGTALASLGWAQGSNFSPEAWLQQYGYLP...  reviewed      1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ==========================================\n",
    "# Load stored UniProt CSV -> clean -> balance -> split\n",
    "# ==========================================\n",
    "\n",
    "DATA_PATH = \"uniprot_swissprot_enzyme_nonenzyme_FULL.csv\"\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "print(\"Loaded:\", DATA_PATH, \" shape:\", df.shape)\n",
    "\n",
    "# ---- Detect column names robustly ----\n",
    "seq_col = \"Sequence\" if \"Sequence\" in df.columns else \"sequence\"\n",
    "len_col = \"Length\"   if \"Length\"   in df.columns else \"length\"\n",
    "acc_col = \"Entry\" if \"Entry\" in df.columns else (\"Accession\" if \"Accession\" in df.columns else \"accession\")\n",
    "\n",
    "# --------------------------\n",
    "# Cleaning\n",
    "# --------------------------\n",
    "df[seq_col] = df[seq_col].astype(str).str.strip()\n",
    "df = df[df[seq_col].notna() & (df[seq_col].str.len() > 0)].copy()\n",
    "\n",
    "df[len_col] = pd.to_numeric(df[len_col], errors=\"coerce\")\n",
    "df = df[df[len_col].between(50, 1024)].copy()\n",
    "\n",
    "if acc_col in df.columns:\n",
    "    before = len(df)\n",
    "    df = df.drop_duplicates(subset=[acc_col]).copy()\n",
    "    print(f\"De-duplicated by {acc_col}: {before} -> {len(df)}\")\n",
    "\n",
    "# Keep only standard amino acids (+X allowed)\n",
    "allowed = set(\"ACDEFGHIKLMNPQRSTVWYX\")\n",
    "df = df[df[seq_col].apply(lambda s: set(s).issubset(allowed))].copy()\n",
    "\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(\"\\nAfter cleaning:\", df.shape)\n",
    "print(\"Label counts:\\n\", df[\"label\"].value_counts())\n",
    "\n",
    "# --------------------------\n",
    "# Optional fixed-size balanced benchmark\n",
    "# --------------------------\n",
    "TOTAL_N = 40000  # set to None to keep all data\n",
    "if TOTAL_N is not None:\n",
    "    per_class = TOTAL_N // 2\n",
    "\n",
    "    n_pos = min(per_class, (df[\"label\"] == 1).sum())\n",
    "    n_neg = min(per_class, (df[\"label\"] == 0).sum())\n",
    "\n",
    "    df_pos = df[df[\"label\"] == 1].sample(n=n_pos, random_state=SEED)\n",
    "    df_neg = df[df[\"label\"] == 0].sample(n=n_neg, random_state=SEED)\n",
    "\n",
    "    df = pd.concat([df_pos, df_neg], ignore_index=True)\\\n",
    "           .sample(frac=1.0, random_state=SEED)\\\n",
    "           .reset_index(drop=True)\n",
    "\n",
    "    print(\"\\nAfter balancing:\", df.shape)\n",
    "    print(\"Label counts:\\n\", df[\"label\"].value_counts())\n",
    "\n",
    "# --------------------------\n",
    "# Train/Test split\n",
    "# --------------------------\n",
    "train_df, test_df = train_test_split(\n",
    "    df,\n",
    "    test_size=0.20,\n",
    "    random_state=SEED,\n",
    "    stratify=df[\"label\"]\n",
    ")\n",
    "\n",
    "print(\"\\nTrain:\", train_df.shape, \"Test:\", test_df.shape)\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60dbac67-4b29-427a-a0b0-94d39d30ab37",
   "metadata": {},
   "source": [
    "## ðŸ¤– Inviting ProtT5 to the Lab\n",
    "\n",
    "ProtT5 is larger and heavier than ProtBERT.\n",
    "\n",
    "So Dr. Aris does two things:\n",
    "1. loads it in **half precision** on GPU (if available),\n",
    "2. keeps **batch size small** to avoid CUDA out-of-memory.\n",
    "\n",
    "Then we prepare sequences in the format ProtT5 expects:\n",
    "- uppercase\n",
    "- replace rare amino acids `U, Z, O, B â†’ X`\n",
    "- add spaces between amino acids (token-like format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e7b92e5-dbd0-4b5b-bd75-65afee16373a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded encoder: Rostlab/prot_t5_xl_uniref50\n",
      "Tokenizer fast? False\n",
      "Model running on: cuda | dtype: torch.float16\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# Load ProtT5 model + tokenizer (FIX)\n",
    "# ==========================================\n",
    "\n",
    "MODEL_NAME = \"Rostlab/prot_t5_xl_uniref50\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=False)\n",
    "\n",
    "dtype = torch.float16 if device.type == \"cuda\" else torch.float32\n",
    "\n",
    "# Encoder-only model (no decoder required)\n",
    "try:\n",
    "    model = T5EncoderModel.from_pretrained(MODEL_NAME, dtype=dtype).to(device)\n",
    "except TypeError:\n",
    "    model = T5EncoderModel.from_pretrained(MODEL_NAME, torch_dtype=dtype).to(device)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "print(\"Loaded encoder:\", MODEL_NAME)\n",
    "print(\"Tokenizer fast?\", getattr(tokenizer, \"is_fast\", False))\n",
    "print(\"Model running on:\", device, \"| dtype:\", dtype)\n",
    "\n",
    "# ==========================================\n",
    "# ProtT5 sequence preparation\n",
    "# ==========================================\n",
    "\n",
    "RARE_AA = re.compile(r\"[UZOB]\")\n",
    "\n",
    "def prott5_prepare_sequence(seq: str) -> str:\n",
    "    \"\"\"\n",
    "    Convert sequence to ProtT5 format:\n",
    "    - Uppercase\n",
    "    - Replace rare AAs with X\n",
    "    - Space-separated tokens\n",
    "    \"\"\"\n",
    "    seq = str(seq).strip().upper()\n",
    "    seq = RARE_AA.sub(\"X\", seq)\n",
    "    return \" \".join(list(seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f96fff-c1ca-45ed-ad33-58de72c404a8",
   "metadata": {},
   "source": [
    "## ðŸ§  When ProtT5 Begins to Read\n",
    "\n",
    "Now the model reads each sequence and produces a hidden representation per token.\n",
    "\n",
    "To turn token-level outputs into a single vector per protein, we use:\n",
    "âœ… **mean pooling** over valid (non-padding) tokens.\n",
    "\n",
    "This yields embeddings shaped like:\n",
    "- Train: `(N_train, hidden_dim)`\n",
    "- Test : `(N_test, hidden_dim)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "152fc127-dc39-4a46-b7d6-a3ce7ec04f25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Mean pooling helper\n",
    "# ==========================================\n",
    "\n",
    "@torch.no_grad()\n",
    "def mean_pool(last_hidden_state: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n",
    "    mask = attention_mask.unsqueeze(-1).type_as(last_hidden_state)\n",
    "    summed = (last_hidden_state * mask).sum(dim=1)\n",
    "    counts = mask.sum(dim=1).clamp(min=1e-9)\n",
    "    return summed / counts\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# Embedding extraction (ProtT5)\n",
    "# ==========================================\n",
    "\n",
    "@torch.no_grad()\n",
    "def build_prott5_embeddings(sequences, batch_size=2, max_length=512) -> np.ndarray:\n",
    "    prepared = [prott5_prepare_sequence(s) for s in sequences]\n",
    "    loader = DataLoader(prepared, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    all_embs = []\n",
    "    for batch in loader:\n",
    "        toks = tokenizer(\n",
    "            list(batch),\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=max_length\n",
    "        ).to(device)\n",
    "\n",
    "        out = model(**toks)\n",
    "        pooled = mean_pool(out.last_hidden_state, toks[\"attention_mask\"])\n",
    "\n",
    "        all_embs.append(pooled.float().cpu().numpy())  # ensure fp32 on CPU side\n",
    "\n",
    "    return np.vstack(all_embs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab17f02f-9afe-4a36-8d66-c50f10188583",
   "metadata": {},
   "source": [
    "### Generate ProtT5 Embeddings\n",
    "\n",
    "Dr. Aris will now:\n",
    "- embed train proteins\n",
    "- embed test proteins\n",
    "- collect labels\n",
    "\n",
    "If you hit CUDA memory issues:\n",
    "- reduce `batch_size=2 â†’ 1`\n",
    "- or reduce `max_length=512 â†’ 384`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b377db41-8e8f-4b9c-b3c1-b570a5f09602",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train embeddings: (32000, 1024)\n",
      "Test embeddings : (8000, 1024)\n",
      "Labels shape     : (32000,) (8000,)\n"
     ]
    }
   ],
   "source": [
    "# Detect sequence column (same logic as before)\n",
    "seq_col_train = \"Sequence\" if \"Sequence\" in train_df.columns else \"sequence\"\n",
    "\n",
    "# Build embeddings\n",
    "X_train = build_prott5_embeddings(train_df[seq_col_train].tolist(), batch_size=2, max_length=512)\n",
    "X_test  = build_prott5_embeddings(test_df[seq_col_train].tolist(), batch_size=2, max_length=512)\n",
    "\n",
    "y_train = train_df[\"label\"].to_numpy().astype(int)\n",
    "y_test  = test_df[\"label\"].to_numpy().astype(int)\n",
    "\n",
    "print(\"Train embeddings:\", X_train.shape)\n",
    "print(\"Test embeddings :\", X_test.shape)\n",
    "print(\"Labels shape     :\", y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8932cc-5e25-417d-81f1-00a0b9c27eee",
   "metadata": {},
   "source": [
    "## ðŸ§ª Probing ProtT5â€™s Knowledge\n",
    "\n",
    "We reuse the same evaluation logic as ProtBERT:\n",
    "- ROC-AUC\n",
    "- PR-AUC\n",
    "- F1\n",
    "- Accuracy\n",
    "\n",
    "So performance differences reflect the **embedding quality**, not metric changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "67839ed5-8049-4b5f-b1d3-9da7273cc164",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def eval_binary_classifier(name, clf, X_train, y_train, X_test, y_test):\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Probability-like scores for ROC/PR metrics\n",
    "    if hasattr(clf, \"predict_proba\"):\n",
    "        probs = clf.predict_proba(X_test)[:, 1]\n",
    "    else:\n",
    "        scores = clf.decision_function(X_test)\n",
    "        probs = (scores - scores.min()) / (scores.max() - scores.min() + 1e-9)\n",
    "\n",
    "    preds = (probs >= 0.5).astype(int)\n",
    "\n",
    "    roc = roc_auc_score(y_test, probs)\n",
    "    pr  = average_precision_score(y_test, probs)\n",
    "    f1  = f1_score(y_test, preds)\n",
    "    acc = accuracy_score(y_test, preds)\n",
    "\n",
    "    print(f\"{name:<14} ROC-AUC: {roc:.4f}\")\n",
    "    print(f\"{name:<14} PR-AUC : {pr:.4f}\")\n",
    "    print(f\"{name:<14} F1     : {f1:.4f}\")\n",
    "    print(f\"{name:<14} Acc    : {acc:.4f}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    return {\"model\": name, \"roc_auc\": roc, \"pr_auc\": pr, \"f1\": f1, \"acc\": acc}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb5b27d-5730-4e72-9a0b-2672b4a02e51",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Train & Compare LR / SVM / MLP (ProtT5 Embeddings)\n",
    "\n",
    "Now we perform the same downstream testing we used for ESM-2 and ProtBERT:\n",
    "- **Logistic Regression** (strong linear baseline)\n",
    "- **Linear SVM** (margin-based baseline, calibrated for probabilities)\n",
    "- **MLP** (a small nonlinear head)\n",
    "\n",
    "This keeps the comparison clean across models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b3be43f-fca0-4a9c-8deb-dc3f5f6678c2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ProtT5+LR      ROC-AUC: 0.9827\n",
      "ProtT5+LR      PR-AUC : 0.9788\n",
      "ProtT5+LR      F1     : 0.9459\n",
      "ProtT5+LR      Acc    : 0.9459\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/apps/jupyter/6.5.4/lib/python3.10/site-packages/sklearn/svm/_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "/apps/jupyter/6.5.4/lib/python3.10/site-packages/sklearn/svm/_base.py:1237: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/apps/jupyter/6.5.4/lib/python3.10/site-packages/sklearn/svm/_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "/apps/jupyter/6.5.4/lib/python3.10/site-packages/sklearn/svm/_base.py:1237: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/apps/jupyter/6.5.4/lib/python3.10/site-packages/sklearn/svm/_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "/apps/jupyter/6.5.4/lib/python3.10/site-packages/sklearn/svm/_base.py:1237: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ProtT5+SVM     ROC-AUC: 0.9831\n",
      "ProtT5+SVM     PR-AUC : 0.9791\n",
      "ProtT5+SVM     F1     : 0.9471\n",
      "ProtT5+SVM     Acc    : 0.9470\n",
      "------------------------------------------------------------\n",
      "ProtT5+MLP     ROC-AUC: 0.9946\n",
      "ProtT5+MLP     PR-AUC : 0.9933\n",
      "ProtT5+MLP     F1     : 0.9739\n",
      "ProtT5+MLP     Acc    : 0.9739\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>pr_auc</th>\n",
       "      <th>f1</th>\n",
       "      <th>acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ProtT5+LR</td>\n",
       "      <td>0.982745</td>\n",
       "      <td>0.978827</td>\n",
       "      <td>0.945909</td>\n",
       "      <td>0.945875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ProtT5+SVM</td>\n",
       "      <td>0.983118</td>\n",
       "      <td>0.979120</td>\n",
       "      <td>0.947106</td>\n",
       "      <td>0.947000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ProtT5+MLP</td>\n",
       "      <td>0.994554</td>\n",
       "      <td>0.993313</td>\n",
       "      <td>0.973872</td>\n",
       "      <td>0.973875</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        model   roc_auc    pr_auc        f1       acc\n",
       "0   ProtT5+LR  0.982745  0.978827  0.945909  0.945875\n",
       "1  ProtT5+SVM  0.983118  0.979120  0.947106  0.947000\n",
       "2  ProtT5+MLP  0.994554  0.993313  0.973872  0.973875"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "# ProtT5 + Logistic Regression\n",
    "clf_lr = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"lr\", LogisticRegression(max_iter=3000, n_jobs=-1))\n",
    "])\n",
    "results.append(eval_binary_classifier(\"ProtT5+LR\", clf_lr, X_train, y_train, X_test, y_test))\n",
    "\n",
    "# ProtT5 + Linear SVM (calibrated)\n",
    "base_svm = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"svm\", LinearSVC(max_iter=5000))\n",
    "])\n",
    "clf_svm = CalibratedClassifierCV(base_svm, method=\"sigmoid\", cv=3)\n",
    "results.append(eval_binary_classifier(\"ProtT5+SVM\", clf_svm, X_train, y_train, X_test, y_test))\n",
    "\n",
    "# ProtT5 + MLP\n",
    "clf_mlp = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"mlp\", MLPClassifier(\n",
    "        hidden_layer_sizes=(256, 128),\n",
    "        activation=\"relu\",\n",
    "        alpha=1e-4,\n",
    "        max_iter=200,\n",
    "        early_stopping=True,\n",
    "        n_iter_no_change=10,\n",
    "        random_state=SEED\n",
    "    ))\n",
    "])\n",
    "results.append(eval_binary_classifier(\"ProtT5+MLP\", clf_mlp, X_train, y_train, X_test, y_test))\n",
    "\n",
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6458444-c8bc-4108-8498-8fdc75915573",
   "metadata": {},
   "source": [
    "# ðŸ§­ Reflections from the Expedition\n",
    "\n",
    "Dr. Aris closes the ProtT5 manuscript gently.\n",
    "\n",
    "What we achieved:\n",
    "- We generated **ProtT5 embeddings** for enzyme vs non-enzyme proteins.\n",
    "- We evaluated multiple lightweight classifiers (LR / SVM / MLP) using the **same metrics and split** as earlier notebooks.\n",
    "- This makes it easy to compare representation quality across:\n",
    "  **ESM-2 vs ProtBERT vs ProtT5**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4b3c87-6df8-477d-8b15-d293063059fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

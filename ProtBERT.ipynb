{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5ed10a4-6269-424a-bbf0-15db171d7c33",
   "metadata": {},
   "source": [
    "# ðŸ§¬ A ProtBERT Expedition with Dr. Aris\n",
    "\n",
    "In this notebook, we once again follow **Dr. Aris**, a computational biologist who sees proteins not as mere chemical polymers, but as ancient biological manuscripts â€” sequences written in a molecular language refined by evolution.\n",
    "\n",
    "In previous explorations, Dr. Aris turned to **ESM-2**, a powerful transformer trained to read this language at scale. With ESM-2, we successfully:\n",
    "\n",
    "- Classified proteins as enzyme vs non-enzyme  \n",
    "- Predicted Pfam families  \n",
    "- Observed strong functional signal encoded in sequence embeddings  \n",
    "\n",
    "The results were impressive.\n",
    "\n",
    "But good science never stops at one model.\n",
    "\n",
    "Today, Dr. Aris seeks a second opinion â€” another linguistic scholar trained to interpret protein grammar:\n",
    "\n",
    "**ProtBERT**\n",
    "\n",
    "ProtBERT approaches proteins as sentences composed of amino acid tokens, learning statistical structure from vast sequence corpora.\n",
    "\n",
    "Our mission in this expedition:\n",
    "\n",
    "- Use ProtBERT as a **frozen embedding generator**\n",
    "- Train lightweight downstream classifiers (LR, SVM, MLP)\n",
    "- Evaluate whether ProtBERT embeddings encode functional signal\n",
    "- Compare fairly against our ESM-2 benchmarks\n",
    "\n",
    "This is not about building the deepest neural network.\n",
    "\n",
    "This is about understanding the representation.\n",
    "\n",
    "Does ProtBERT truly grasp the grammar of biological function?\n",
    "\n",
    "Let us find out."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02a711d-0ba1-4f34-9db0-01fd8174a58a",
   "metadata": {},
   "source": [
    "### âš™ï¸ Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adf8484a-97b8-42a8-8a7c-34c38051fafd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on: cuda\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# Global Imports (Single Location)\n",
    "# ================================\n",
    "\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import time\n",
    "from io import StringIO\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    average_precision_score,\n",
    "    f1_score,\n",
    "    accuracy_score\n",
    ")\n",
    "\n",
    "# ----------------\n",
    "# Environment setup\n",
    "# ----------------\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Running on:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d0e157-4b02-484c-a04e-0c762b85c5ef",
   "metadata": {},
   "source": [
    "## ðŸ“‚ Returning to the Library of Proteins\n",
    "\n",
    "Dr. Aris does not redownload knowledge twice.\n",
    "\n",
    "In our previous expedition, we carefully retrieved and stored a curated Swiss-Prot dataset from UniProt â€” a collection of proteins filtered for quality, completeness, and functional annotation.\n",
    "\n",
    "Now, instead of querying the vast biological archive again, we simply reopen the manuscript we preserved.\n",
    "\n",
    "In this step, Dr. Aris will:\n",
    "\n",
    "- Load the stored UniProt dataset\n",
    "- Remove incomplete or invalid sequences\n",
    "- Enforce length constraints suitable for transformer models\n",
    "- Optionally create a balanced benchmark subset\n",
    "- Split the data into training and testing sets\n",
    "\n",
    "The goal is simple:\n",
    "\n",
    "Prepare a clean and reproducible foundation upon which ProtBERT can begin its linguistic analysis of proteins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b90522e-3b5b-4245-b90e-7b19a53ff0f2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/local/25278809/ipykernel_3304006/2353240010.py:6: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(DATA_PATH)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: uniprot_swissprot_enzyme_nonenzyme_FULL.csv  shape: (537346, 9)\n",
      "De-duplicated by Entry: 537346 -> 537346\n",
      "\n",
      "After cleaning: (536963, 9)\n",
      "Label counts:\n",
      " label\n",
      "0    270121\n",
      "1    266842\n",
      "Name: count, dtype: int64\n",
      "\n",
      "After balancing: (40000, 9)\n",
      "Label counts:\n",
      " label\n",
      "0    20000\n",
      "1    20000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Train: (32000, 9) Test: (8000, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entry</th>\n",
       "      <th>Entry Name</th>\n",
       "      <th>Protein names</th>\n",
       "      <th>Organism</th>\n",
       "      <th>Length</th>\n",
       "      <th>EC number</th>\n",
       "      <th>Sequence</th>\n",
       "      <th>Reviewed</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17005</th>\n",
       "      <td>P74737</td>\n",
       "      <td>RECA_SYNY3</td>\n",
       "      <td>Protein RecA (Recombinase A)</td>\n",
       "      <td>Synechocystis sp. (strain ATCC 27184 / PCC 680...</td>\n",
       "      <td>354</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MASTNISDREKALNAALAQIERSFGKGAIMRLGDATQMRVETISTG...</td>\n",
       "      <td>reviewed</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19930</th>\n",
       "      <td>A1WXM7</td>\n",
       "      <td>SAHH_HALHL</td>\n",
       "      <td>Adenosylhomocysteinase (EC 3.13.2.1) (S-adenos...</td>\n",
       "      <td>Halorhodospira halophila (strain DSM 244 / SL1...</td>\n",
       "      <td>430</td>\n",
       "      <td>3.13.2.1</td>\n",
       "      <td>MSNQDYKVADISLADWGRKEIKIAESEMPGLMETRREFAAQKPLKG...</td>\n",
       "      <td>reviewed</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8641</th>\n",
       "      <td>B7ZR65</td>\n",
       "      <td>SOX9A_XENLA</td>\n",
       "      <td>Transcription factor Sox-9-A</td>\n",
       "      <td>Xenopus laevis (African clawed frog)</td>\n",
       "      <td>477</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MNLLDPFMKMTEEQDKCMSGAPSPTMSDDSAGSPCPSGSGSDTENT...</td>\n",
       "      <td>reviewed</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35026</th>\n",
       "      <td>P07435</td>\n",
       "      <td>OBP_BOVIN</td>\n",
       "      <td>Odorant-binding protein (OBP) (Olfactory mucos...</td>\n",
       "      <td>Bos taurus (Bovine)</td>\n",
       "      <td>159</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AQEEEAEQNLSELSGPWRTVYIGSTNPEKIQENGPFRTYFRELVFD...</td>\n",
       "      <td>reviewed</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19704</th>\n",
       "      <td>P53690</td>\n",
       "      <td>MMP14_MOUSE</td>\n",
       "      <td>Matrix metalloproteinase-14 (MMP-14) (EC 3.4.2...</td>\n",
       "      <td>Mus musculus (Mouse)</td>\n",
       "      <td>582</td>\n",
       "      <td>3.4.24.80</td>\n",
       "      <td>MSPAPRPSRSLLLPLLTLGTALASLGWAQGSNFSPEAWLQQYGYLP...</td>\n",
       "      <td>reviewed</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Entry   Entry Name                                      Protein names  \\\n",
       "17005  P74737   RECA_SYNY3                       Protein RecA (Recombinase A)   \n",
       "19930  A1WXM7   SAHH_HALHL  Adenosylhomocysteinase (EC 3.13.2.1) (S-adenos...   \n",
       "8641   B7ZR65  SOX9A_XENLA                       Transcription factor Sox-9-A   \n",
       "35026  P07435    OBP_BOVIN  Odorant-binding protein (OBP) (Olfactory mucos...   \n",
       "19704  P53690  MMP14_MOUSE  Matrix metalloproteinase-14 (MMP-14) (EC 3.4.2...   \n",
       "\n",
       "                                                Organism  Length  EC number  \\\n",
       "17005  Synechocystis sp. (strain ATCC 27184 / PCC 680...     354        NaN   \n",
       "19930  Halorhodospira halophila (strain DSM 244 / SL1...     430   3.13.2.1   \n",
       "8641                Xenopus laevis (African clawed frog)     477        NaN   \n",
       "35026                                Bos taurus (Bovine)     159        NaN   \n",
       "19704                               Mus musculus (Mouse)     582  3.4.24.80   \n",
       "\n",
       "                                                Sequence  Reviewed  label  \n",
       "17005  MASTNISDREKALNAALAQIERSFGKGAIMRLGDATQMRVETISTG...  reviewed      0  \n",
       "19930  MSNQDYKVADISLADWGRKEIKIAESEMPGLMETRREFAAQKPLKG...  reviewed      1  \n",
       "8641   MNLLDPFMKMTEEQDKCMSGAPSPTMSDDSAGSPCPSGSGSDTENT...  reviewed      0  \n",
       "35026  AQEEEAEQNLSELSGPWRTVYIGSTNPEKIQENGPFRTYFRELVFD...  reviewed      0  \n",
       "19704  MSPAPRPSRSLLLPLLTLGTALASLGWAQGSNFSPEAWLQQYGYLP...  reviewed      1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ==========================================\n",
    "# Load stored UniProt CSV -> clean -> balance -> split\n",
    "# ==========================================\n",
    "\n",
    "DATA_PATH = \"uniprot_swissprot_enzyme_nonenzyme_FULL.csv\"\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "print(\"Loaded:\", DATA_PATH, \" shape:\", df.shape)\n",
    "\n",
    "# ---- Detect column names robustly ----\n",
    "seq_col = \"Sequence\" if \"Sequence\" in df.columns else \"sequence\"\n",
    "len_col = \"Length\"   if \"Length\"   in df.columns else \"length\"\n",
    "acc_col = \"Entry\" if \"Entry\" in df.columns else (\"Accession\" if \"Accession\" in df.columns else \"accession\")\n",
    "\n",
    "# --------------------------\n",
    "# Cleaning\n",
    "# --------------------------\n",
    "df[seq_col] = df[seq_col].astype(str).str.strip()\n",
    "df = df[df[seq_col].notna() & (df[seq_col].str.len() > 0)].copy()\n",
    "\n",
    "df[len_col] = pd.to_numeric(df[len_col], errors=\"coerce\")\n",
    "df = df[df[len_col].between(50, 1024)].copy()\n",
    "\n",
    "if acc_col in df.columns:\n",
    "    before = len(df)\n",
    "    df = df.drop_duplicates(subset=[acc_col]).copy()\n",
    "    print(f\"De-duplicated by {acc_col}: {before} -> {len(df)}\")\n",
    "\n",
    "# Keep only standard amino acids (+X allowed)\n",
    "allowed = set(\"ACDEFGHIKLMNPQRSTVWYX\")\n",
    "df = df[df[seq_col].apply(lambda s: set(s).issubset(allowed))].copy()\n",
    "\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(\"\\nAfter cleaning:\", df.shape)\n",
    "print(\"Label counts:\\n\", df[\"label\"].value_counts())\n",
    "\n",
    "# --------------------------\n",
    "# Optional fixed-size balanced benchmark\n",
    "# --------------------------\n",
    "TOTAL_N = 40000  # set to None to keep all data\n",
    "if TOTAL_N is not None:\n",
    "    per_class = TOTAL_N // 2\n",
    "\n",
    "    n_pos = min(per_class, (df[\"label\"] == 1).sum())\n",
    "    n_neg = min(per_class, (df[\"label\"] == 0).sum())\n",
    "\n",
    "    df_pos = df[df[\"label\"] == 1].sample(n=n_pos, random_state=SEED)\n",
    "    df_neg = df[df[\"label\"] == 0].sample(n=n_neg, random_state=SEED)\n",
    "\n",
    "    df = pd.concat([df_pos, df_neg], ignore_index=True)\\\n",
    "           .sample(frac=1.0, random_state=SEED)\\\n",
    "           .reset_index(drop=True)\n",
    "\n",
    "    print(\"\\nAfter balancing:\", df.shape)\n",
    "    print(\"Label counts:\\n\", df[\"label\"].value_counts())\n",
    "\n",
    "# --------------------------\n",
    "# Train/Test split\n",
    "# --------------------------\n",
    "train_df, test_df = train_test_split(\n",
    "    df,\n",
    "    test_size=0.20,\n",
    "    random_state=SEED,\n",
    "    stratify=df[\"label\"]\n",
    ")\n",
    "\n",
    "print(\"\\nTrain:\", train_df.shape, \"Test:\", test_df.shape)\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e027f50a-c980-43ff-8ef4-eac12661ed7f",
   "metadata": {},
   "source": [
    "## ðŸ¤– Inviting ProtBERT to Read the Manuscripts\n",
    "\n",
    "With the protein manuscripts cleaned, filtered, and carefully split into training and testing collections,  \n",
    "Dr. Aris now turns to a new linguistic scholar of biology:\n",
    "\n",
    "**ProtBERT**\n",
    "\n",
    "Unlike traditional bioinformatics models, ProtBERT was trained to read proteins as if they were sentences â€”  \n",
    "learning grammar, structure, and statistical patterns directly from vast sequence corpora.\n",
    "\n",
    "But ProtBERT has its own reading conventions:\n",
    "\n",
    "- Amino acids must be uppercase  \n",
    "- Rare amino acids (`U`, `Z`, `O`, `B`) must be replaced with `X`  \n",
    "- Sequences must be tokenized as space-separated letters  \n",
    "  - Example: `\"MKT...\"` â†’ `\"M K T ...\"`\n",
    "\n",
    "Before ProtBERT can begin interpreting biological meaning,  \n",
    "Dr. Aris must first prepare the sequences in the format ProtBERT understands."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c549954b-2c09-40f5-a5aa-0aa65b8214da",
   "metadata": {},
   "source": [
    "### Load ProtBERT + Define Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19353dc0-4eae-435b-addf-9a4e03b464bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping import of cpp extensions due to incompatible torch version 2.9.1+cu128 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: Rostlab/prot_bert\n",
      "Model running on: cuda\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# Load ProtBERT model + tokenizer\n",
    "# ==========================================\n",
    "\n",
    "MODEL_NAME = \"Rostlab/prot_bert\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, do_lower_case=False)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME).to(device)\n",
    "model.eval()\n",
    "\n",
    "print(\"Loaded:\", MODEL_NAME)\n",
    "print(\"Model running on:\", device)\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# ProtBERT sequence preparation\n",
    "# ==========================================\n",
    "\n",
    "RARE_AA = re.compile(r\"[UZOB]\")\n",
    "\n",
    "def protbert_prepare_sequence(seq: str) -> str:\n",
    "    \"\"\"\n",
    "    Convert sequence to ProtBERT format:\n",
    "    - Uppercase\n",
    "    - Replace rare AAs with X\n",
    "    - Space-separated tokens\n",
    "    \"\"\"\n",
    "    seq = str(seq).strip().upper()\n",
    "    seq = RARE_AA.sub(\"X\", seq)\n",
    "    return \" \".join(list(seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde9057b-ced2-4754-a7b6-8c0db0e56b8d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## ðŸ§  When ProtBERT Begins to Read\n",
    "\n",
    "The manuscripts are prepared.  \n",
    "The scholar is ready.\n",
    "\n",
    "ProtBERT does not classify proteins directly.  \n",
    "Instead, it transforms each sequence into a dense numerical representation â€”  \n",
    "a **1024-dimensional embedding** that captures structural and functional patterns learned during pretraining.\n",
    "\n",
    "In this step, Dr. Aris will:\n",
    "\n",
    "- Tokenize each protein sequence\n",
    "- Run a forward pass through ProtBERT\n",
    "- Apply masked mean pooling over token embeddings\n",
    "- Convert each protein into a fixed-length vector\n",
    "\n",
    "These embeddings will serve as the foundation for downstream classifiers.\n",
    "\n",
    "This is where language becomes representation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d901b7d-ed00-4fa5-a56e-b5907f065e4c",
   "metadata": {},
   "source": [
    "### Generate ProtBERT Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92e56adb-f125-4532-a744-8e0e64c4962b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train embeddings: (32000, 1024)\n",
      "Test embeddings : (8000, 1024)\n",
      "Labels shape     : (32000,) (8000,)\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# Mean pooling helper\n",
    "# ==========================================\n",
    "\n",
    "@torch.no_grad()\n",
    "def mean_pool(last_hidden_state: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n",
    "    mask = attention_mask.unsqueeze(-1).type_as(last_hidden_state)\n",
    "    summed = (last_hidden_state * mask).sum(dim=1)\n",
    "    counts = mask.sum(dim=1).clamp(min=1e-9)\n",
    "    return summed / counts\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# Embedding extraction\n",
    "# ==========================================\n",
    "\n",
    "@torch.no_grad()\n",
    "def build_protbert_embeddings(sequences, batch_size=8, max_length=512) -> np.ndarray:\n",
    "    prepared = [protbert_prepare_sequence(s) for s in sequences]\n",
    "    loader = DataLoader(prepared, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    all_embs = []\n",
    "    for batch in loader:\n",
    "        toks = tokenizer(\n",
    "            list(batch),\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=max_length\n",
    "        ).to(device)\n",
    "\n",
    "        out = model(**toks)\n",
    "        pooled = mean_pool(out.last_hidden_state, toks[\"attention_mask\"])\n",
    "        all_embs.append(pooled.cpu().numpy())\n",
    "\n",
    "    return np.vstack(all_embs)\n",
    "\n",
    "\n",
    "# Detect sequence column\n",
    "seq_col_train = \"Sequence\" if \"Sequence\" in train_df.columns else \"sequence\"\n",
    "\n",
    "# Build embeddings\n",
    "X_train = build_protbert_embeddings(train_df[seq_col_train].tolist(), batch_size=8)\n",
    "X_test  = build_protbert_embeddings(test_df[seq_col_train].tolist(), batch_size=8)\n",
    "\n",
    "y_train = train_df[\"label\"].to_numpy().astype(int)\n",
    "y_test  = test_df[\"label\"].to_numpy().astype(int)\n",
    "\n",
    "print(\"Train embeddings:\", X_train.shape)\n",
    "print(\"Test embeddings :\", X_test.shape)\n",
    "print(\"Labels shape     :\", y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea57ef3c-6788-4e6d-83be-8f7d9732996d",
   "metadata": {},
   "source": [
    "## ðŸ§ª Probing ProtBERTâ€™s Knowledge\n",
    "\n",
    "ProtBERT has now converted every protein into a fixed-length embedding.\n",
    "\n",
    "But embeddings alone do not answer our scientific question.\n",
    "\n",
    "Dr. Aris now performs a simple but powerful test:\n",
    "\n",
    "If ProtBERT truly encodes functional meaning, then even **lightweight classifiers** should be able to separate:\n",
    "\n",
    "- **Enzymes** (label = 1)\n",
    "- **Non-enzymes** (label = 0)\n",
    "\n",
    "So instead of training a huge deep network, we use small downstream models:\n",
    "\n",
    "- Logistic Regression (linear probe)\n",
    "- Linear SVM (margin-based probe)\n",
    "- MLP (nonlinear probe)\n",
    "\n",
    "This is not about building the most complex model.\n",
    "\n",
    "This is about evaluating the representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e5cbfc5-bd54-48ff-8e9e-499e53e58d0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def eval_binary_classifier(name, clf, X_train, y_train, X_test, y_test):\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Probability-like scores for ROC/PR metrics\n",
    "    if hasattr(clf, \"predict_proba\"):\n",
    "        probs = clf.predict_proba(X_test)[:, 1]\n",
    "    else:\n",
    "        scores = clf.decision_function(X_test)\n",
    "        probs = (scores - scores.min()) / (scores.max() - scores.min() + 1e-9)\n",
    "\n",
    "    preds = (probs >= 0.5).astype(int)\n",
    "\n",
    "    roc = roc_auc_score(y_test, probs)\n",
    "    pr  = average_precision_score(y_test, probs)\n",
    "    f1  = f1_score(y_test, preds)\n",
    "    acc = accuracy_score(y_test, preds)\n",
    "\n",
    "    print(f\"{name:<14} ROC-AUC: {roc:.4f}\")\n",
    "    print(f\"{name:<14} PR-AUC : {pr:.4f}\")\n",
    "    print(f\"{name:<14} F1     : {f1:.4f}\")\n",
    "    print(f\"{name:<14} Acc    : {acc:.4f}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    return {\"model\": name, \"roc_auc\": roc, \"pr_auc\": pr, \"f1\": f1, \"acc\": acc}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33aefef-7336-4d0d-ac05-26b0431147b1",
   "metadata": {},
   "source": [
    "### Train & Compare LR / SVM / MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dfcda0a4-8af4-4c17-8aa7-265336a887db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ProtBERT+LR    ROC-AUC: 0.9729\n",
      "ProtBERT+LR    PR-AUC : 0.9709\n",
      "ProtBERT+LR    F1     : 0.9188\n",
      "ProtBERT+LR    Acc    : 0.9191\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/apps/jupyter/6.5.4/lib/python3.10/site-packages/sklearn/svm/_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "/apps/jupyter/6.5.4/lib/python3.10/site-packages/sklearn/svm/_base.py:1237: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/apps/jupyter/6.5.4/lib/python3.10/site-packages/sklearn/svm/_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "/apps/jupyter/6.5.4/lib/python3.10/site-packages/sklearn/svm/_base.py:1237: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/apps/jupyter/6.5.4/lib/python3.10/site-packages/sklearn/svm/_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "/apps/jupyter/6.5.4/lib/python3.10/site-packages/sklearn/svm/_base.py:1237: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ProtBERT+SVM   ROC-AUC: 0.9730\n",
      "ProtBERT+SVM   PR-AUC : 0.9711\n",
      "ProtBERT+SVM   F1     : 0.9182\n",
      "ProtBERT+SVM   Acc    : 0.9186\n",
      "------------------------------------------------------------\n",
      "ProtBERT+MLP   ROC-AUC: 0.9895\n",
      "ProtBERT+MLP   PR-AUC : 0.9885\n",
      "ProtBERT+MLP   F1     : 0.9551\n",
      "ProtBERT+MLP   Acc    : 0.9555\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>pr_auc</th>\n",
       "      <th>f1</th>\n",
       "      <th>acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ProtBERT+LR</td>\n",
       "      <td>0.972919</td>\n",
       "      <td>0.970926</td>\n",
       "      <td>0.918770</td>\n",
       "      <td>0.919125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ProtBERT+SVM</td>\n",
       "      <td>0.972957</td>\n",
       "      <td>0.971094</td>\n",
       "      <td>0.918247</td>\n",
       "      <td>0.918625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ProtBERT+MLP</td>\n",
       "      <td>0.989535</td>\n",
       "      <td>0.988521</td>\n",
       "      <td>0.955051</td>\n",
       "      <td>0.955500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          model   roc_auc    pr_auc        f1       acc\n",
       "0   ProtBERT+LR  0.972919  0.970926  0.918770  0.919125\n",
       "1  ProtBERT+SVM  0.972957  0.971094  0.918247  0.918625\n",
       "2  ProtBERT+MLP  0.989535  0.988521  0.955051  0.955500"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "# ProtBERT + Logistic Regression\n",
    "clf_lr = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"lr\", LogisticRegression(max_iter=3000, n_jobs=-1))\n",
    "])\n",
    "results.append(eval_binary_classifier(\"ProtBERT+LR\", clf_lr, X_train, y_train, X_test, y_test))\n",
    "\n",
    "# ProtBERT + Linear SVM (calibrated)\n",
    "base_svm = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"svm\", LinearSVC(max_iter=5000))\n",
    "])\n",
    "clf_svm = CalibratedClassifierCV(base_svm, method=\"sigmoid\", cv=3)\n",
    "results.append(eval_binary_classifier(\"ProtBERT+SVM\", clf_svm, X_train, y_train, X_test, y_test))\n",
    "\n",
    "# ProtBERT + MLP\n",
    "clf_mlp = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"mlp\", MLPClassifier(\n",
    "        hidden_layer_sizes=(256, 128),\n",
    "        activation=\"relu\",\n",
    "        alpha=1e-4,\n",
    "        max_iter=200,\n",
    "        early_stopping=True,\n",
    "        n_iter_no_change=10,\n",
    "        random_state=SEED\n",
    "    ))\n",
    "])\n",
    "results.append(eval_binary_classifier(\"ProtBERT+MLP\", clf_mlp, X_train, y_train, X_test, y_test))\n",
    "\n",
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cece2918-4c11-4a6a-8e3a-386453758355",
   "metadata": {},
   "source": [
    "# ðŸ§­ Reflections from the Expedition\n",
    "\n",
    "ProtBERT has spoken.\n",
    "\n",
    "From raw amino acid sequences â€” nothing more than strings of letters â€” it constructed dense representations that allowed simple classifiers to distinguish enzymes from non-enzymes with strong performance.\n",
    "\n",
    "This is the remarkable part.\n",
    "\n",
    "We did not fine-tune ProtBERT.\n",
    "We did not modify its internal weights.\n",
    "We merely asked it to *encode*.\n",
    "\n",
    "And yet, the embeddings carried functional signal.\n",
    "\n",
    "For Dr. Aris, this confirms an important insight:\n",
    "\n",
    "Protein language models are not memorizing sequences â€”  \n",
    "they are learning structure in biological grammar.\n",
    "\n",
    "How does ProtBERT compare to ESM-2?\n",
    "\n",
    "- If performance is similar, it suggests convergent representation learning across architectures.\n",
    "- If ESM-2 performs better, it may reflect architectural scaling or training corpus differences.\n",
    "- If ProtBERT competes closely, it reinforces the robustness of transformer-based protein embeddings.\n",
    "\n",
    "But the deeper takeaway is not which model wins.\n",
    "\n",
    "It is this:\n",
    "\n",
    "Functional meaning is encoded in sequence alone.\n",
    "\n",
    "Transformers, when trained at scale, can uncover that meaning.\n",
    "\n",
    "And now, Dr. Aris has two scholars in his library \n",
    "ESM-2 and ProtBERT â€”  \n",
    "ready for future expeditions into biological language."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ee688bd-99b8-482f-b356-3979af927394",
   "metadata": {},
   "source": [
    "# Transformers for Biological Sequence Analysis\n",
    "\n",
    "## Understanding Protein Sequences, Families, and the Usage of ESM-2\n",
    "\n",
    "---\n",
    "\n",
    "## 1. What Are Protein Sequences?\n",
    "\n",
    "Proteins are biological macromolecules composed of amino acids arranged in a linear sequence.\n",
    "Each protein sequence can be represented as a string of characters, where each character corresponds to one of the 20 standard amino acids.\n",
    "\n",
    "**Example sequence:** `MKTLLILTCLVAVALARPKA...`\n",
    "\n",
    "This linear sequence determines:\n",
    "\n",
    "* The three-dimensional structure of the protein\n",
    "* Its biochemical function\n",
    "* Its evolutionary relationships\n",
    "\n",
    "Understanding how sequence relates to structure and function is a central problem in molecular biology.\n",
    "\n",
    "\n",
    "## 2. What Are Protein Families?\n",
    "\n",
    "Protein families group together proteins that:\n",
    "\n",
    "* Share evolutionary ancestry\n",
    "* Have similar structural domains\n",
    "* Perform similar biological functions\n",
    "\n",
    "Databases such as **Pfam** classify proteins into families based on conserved sequence motifs and Hidden Markov Models (HMMs).\n",
    "\n",
    "**Examples of protein families**\n",
    "\n",
    "* **Kinases** — enzymes that transfer phosphate groups\n",
    "* **ABC transporters** — membrane transport proteins\n",
    "* **Response regulators** — signaling proteins\n",
    "\n",
    "Traditionally, identifying family membership required:\n",
    "\n",
    "* Sequence alignment (e.g., BLAST)\n",
    "* Multiple sequence alignment\n",
    "* Profile HMMs\n",
    "\n",
    "These approaches depend heavily on sequence similarity.\n",
    "\n",
    "\n",
    "## 3. What Is ESM-2?\n",
    "\n",
    "**ESM-2 (Evolutionary Scale Modeling 2)** is a transformer-based protein language model trained on millions of protein sequences.\n",
    "\n",
    "Instead of using alignments, ESM-2 learns directly from raw amino acid sequences using a self-supervised objective known as **masked language modeling**.\n",
    "\n",
    "Through this training process, the model learns to:\n",
    "\n",
    "* Capture long-range dependencies in sequences\n",
    "* Encode structural information\n",
    "* Represent functional similarity\n",
    "* Embed evolutionary relationships\n",
    "\n",
    "The output of ESM-2 is a dense numerical representation (**embedding**) of each protein sequence.\n",
    "\n",
    "## There are several ESM-2 checkpoints with differing model sizes. \n",
    "Larger models will generally have better accuracy, but   they require more GPU memory and will take much longer to train. The available ESM-2 checkpoints are:\n",
    "\n",
    "| Checkpoint name        | Num layers | Num parameters |\n",
    "|------------------------|-----------:|---------------:|\n",
    "| esm2_t48_15B_UR50D     | 48         | 15B            |\n",
    "| esm2_t36_3B_UR50D      | 36         | 3B             |\n",
    "| esm2_t33_650M_UR50D    | 33         | 650M           |\n",
    "| esm2_t30_150M_UR50D    | 30         | 150M           |\n",
    "| esm2_t12_35M_UR50D     | 12         | 35M            |\n",
    "| esm2_t6_8M_UR50D       | 6          | 8M             |\n",
    "\n",
    "Note that the larger checkpoints may be very difficult to train without a large cloud GPU like an A100 or H100, and the largest 15B parameter checkpoint will probably be impossible to train on any single GPU! Also, note that memory usage for attention during training will scale as O(batch_size * num_layers * seq_len^2), so larger models on long sequences will use quite a lot of memory! We will use the esm2_t30_150M_UR50D checkpoint for this notebook, which should train on any notebook or modern GPU.\n",
    "\n",
    "## 4. How Will We Use ESM-2 in This Notebook?\n",
    "\n",
    "In this study notebook, we will:\n",
    "\n",
    "1. Retrieve raw protein sequences from **UniProt**\n",
    "2. Convert each sequence into a fixed-length embedding using ESM-2\n",
    "3. Train a simple linear classifier to predict protein family labels\n",
    "4. Evaluate how well the embeddings capture biologically meaningful structure\n",
    "\n",
    "Importantly, we will **not fine-tune ESM-2**.\n",
    "We will use it strictly as a feature extractor and test whether family-level biological information is already encoded in its representations.\n",
    "\n",
    "\n",
    "## 5. Why Is This Important?\n",
    "\n",
    "If a simple linear classifier can accurately predict protein families from ESM-2 embeddings, it suggests that:\n",
    "\n",
    "* The model has learned biologically meaningful structure\n",
    "* No alignment or handcrafted biological features are required\n",
    "* Protein function and evolutionary information are encoded directly in the embedding space\n",
    "\n",
    "This approach demonstrates how transformer-based models can serve as powerful tools for biological discovery.\n",
    "\n",
    "\n",
    "In the following sections, we transition from biological background to practical implementation and experimentation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f662d4-2e06-40a5-8310-91cc06fc6228",
   "metadata": {},
   "source": [
    "## Verify Installation + GPU Availability\n",
    "\n",
    "We verify that libraries import correctly and check whether CUDA is available.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8af1b7c1-6c02-4516-a92e-b8e57aede25d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CUDA available: True\n",
      "GPU: NVIDIA L4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import requests\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import sklearn\n",
    "import matplotlib\n",
    "\n",
    "# Configuration class to store experiment settings in one place\n",
    "@dataclass\n",
    "class Config:\n",
    "    cache_dir: str = \"./data/cache_uniprot\"                      # folder to store downloaded/cached UniProt data\n",
    "    seed: int = 42                                               # random seed for reproducibility\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\" # automatically use GPU if available, otherwise fallback to CPU\n",
    "\n",
    "cfg = Config()\n",
    "\n",
    "os.makedirs(cfg.cache_dir, exist_ok=True)\n",
    "np.random.seed(cfg.seed)\n",
    "torch.manual_seed(cfg.seed)\n",
    "\n",
    "print(\"\\nCUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4dd1520c-c0a5-483a-a5cc-73eb2fdbc5e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ---------- USER CONTROLS ----------\n",
    "TARGET_FAMILIES = 200          # <-- set desired number of families (e.g., 100, 200, 500)\n",
    "PER_CLASS = 150                # sequences per family (balanced)\n",
    "MIN_AFTER_CLEAN = 80           # drop families that become too small after cleaning\n",
    "SCAN_PFAM_MAX = 20000          # how far to scan PFxxxxx IDs (increase if you want more choices)\n",
    "\n",
    "REVIEWED_ONLY = True           # True = Swiss-Prot only (cleaner, smaller). False = bigger, noisier.\n",
    "LENGTH_MIN = 50\n",
    "LENGTH_MAX = 1200\n",
    "\n",
    "# Model choice\n",
    "ESM_MODEL_NAME = \"facebook/esm2_t30_150M_UR50D\"   # <-- your requested model\n",
    "MAX_LEN = 512                 # 512 is fast; 1024 is heavier\n",
    "BATCH_SIZE = 4                # t30_150M needs smaller batch on many GPUs (try 4, 2, or 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95d64d5-bcc5-40db-912e-195f7096e6a0",
   "metadata": {},
   "source": [
    "## Retrieving Protein Sequences from UniProt (Programmatic Access)\n",
    "\n",
    "To build our dataset, we need to retrieve protein sequences directly from UniProt.\n",
    "\n",
    "UniProt provides a REST API that allows us to:\n",
    "- Query proteins using structured search filters\n",
    "- Retrieve results in different formats (TSV, JSON, FASTA)\n",
    "- Handle pagination when results exceed a single page\n",
    "\n",
    "The code below implements a small utility for downloading protein data in TSV format.\n",
    "\n",
    "### Key Components\n",
    "\n",
    "**1. UNIPROT_SEARCH_URL**\n",
    "\n",
    "This is the base REST endpoint for searching UniProtKB:\n",
    "https://rest.uniprot.org/uniprotkb/search\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8cf61739-035e-4490-ac98-73ac0f082caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Optional\n",
    "\n",
    "UNIPROT_SEARCH_URL = \"https://rest.uniprot.org/uniprotkb/search\"\n",
    "\n",
    "def _next_link_from_headers(headers: Dict[str, str]) -> Optional[str]:\n",
    "    link = headers.get(\"Link\")\n",
    "    if not link:\n",
    "        return None\n",
    "    m = re.search(r'<([^>]+)>;\\s*rel=\"next\"', link)\n",
    "    return m.group(1) if m else None\n",
    "\n",
    "def fetch_uniprot_tsv(query: str, fields: str, size: int = 500, max_rows: int = 1000) -> pd.DataFrame:\n",
    "    params = {\"query\": query, \"format\": \"tsv\", \"fields\": fields, \"size\": size}\n",
    "    rows = []\n",
    "    url = UNIPROT_SEARCH_URL\n",
    "\n",
    "    header = None\n",
    "    while url and len(rows) < max_rows:\n",
    "        r = requests.get(url, params=params if url == UNIPROT_SEARCH_URL else None, timeout=60)\n",
    "        r.raise_for_status()\n",
    "\n",
    "        text = r.text.strip()\n",
    "        if not text:\n",
    "            break\n",
    "\n",
    "        lines = text.splitlines()\n",
    "        header = lines[0].split(\"\\t\")\n",
    "        for line in lines[1:]:\n",
    "            rows.append(line.split(\"\\t\"))\n",
    "            if len(rows) >= max_rows:\n",
    "                break\n",
    "\n",
    "        url = _next_link_from_headers(r.headers)\n",
    "        params = None\n",
    "        time.sleep(0.1)\n",
    "\n",
    "    if not rows:\n",
    "        return pd.DataFrame(columns=fields.split(\",\"))\n",
    "    return pd.DataFrame(rows, columns=header)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a30d9cb9-4289-4050-aa0d-d409681c2807",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def uniprot_count(query: str) -> int:\n",
    "    \"\"\"\n",
    "    Returns total number of UniProt entries matching a query\n",
    "    without downloading full records.\n",
    "    \"\"\"\n",
    "    r = requests.get(\n",
    "        UNIPROT_SEARCH_URL,\n",
    "        params={\"query\": query, \"format\": \"json\", \"size\": 0},\n",
    "        timeout=30\n",
    "    )\n",
    "    if r.status_code != 200:\n",
    "        return 0\n",
    "\n",
    "    return int(r.headers.get(\"x-total-results\", \"0\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054d9741-0ecb-4c95-bfb7-8b255069fd92",
   "metadata": {},
   "source": [
    "## Selecting Protein Families for the Benchmark\n",
    "\n",
    "In this step, we identify Pfam families that contain enough protein sequences\n",
    "to support a balanced multi-class classification task.\n",
    "\n",
    "Rather than manually choosing families, we automatically scan Pfam identifiers\n",
    "(PF00001, PF00002, PF00003, ...) and check how many sequences are available\n",
    "in UniProt under our filtering criteria.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Building a Structured UniProt Query\n",
    "\n",
    "The function `build_query_for_pfam()` constructs a structured search query\n",
    "for a given Pfam ID.\n",
    "\n",
    "Each query enforces the following constraints:\n",
    "\n",
    "- `xref:pfam-PFxxxxx` → Protein must belong to a specific Pfam family\n",
    "- `fragment:false` → Exclude incomplete protein fragments\n",
    "- `reviewed:true` (optional) → Include only Swiss-Prot curated entries\n",
    "- `length:[min TO max]` → Restrict protein length range\n",
    "\n",
    "These filters ensure:\n",
    "- High-quality sequences\n",
    "- Reasonable length distributions\n",
    "- Reduced noise in downstream analysis\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Discovering Candidate Families\n",
    "\n",
    "We then iterate over Pfam IDs and:\n",
    "\n",
    "1. Construct a query for each family\n",
    "2. Count how many sequences satisfy our constraints\n",
    "3. Keep only families with at least `PER_CLASS` sequences\n",
    "\n",
    "This guarantees that each selected family has sufficient data\n",
    "to form a balanced classification dataset.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Why We Select More Than Needed\n",
    "\n",
    "We intentionally collect more families than our final target:\n",
    "\n",
    "This buffer accounts for later cleaning steps, such as:\n",
    "\n",
    "- Removing duplicate sequences\n",
    "- Removing multi-label proteins\n",
    "- Dropping families that shrink below a minimum size\n",
    "\n",
    "This ensures that after cleaning, we still retain\n",
    "the desired number of families.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Safety Check\n",
    "\n",
    "The final assertion ensures that we discovered enough families.\n",
    "If not, the user may need to:\n",
    "\n",
    "- Disable the reviewed-only filter\n",
    "- Reduce the required sequences per family\n",
    "- Increase the Pfam search range\n",
    "\n",
    "---\n",
    "\n",
    "### Why This Step Is Important\n",
    "\n",
    "Protein families in UniProt are highly imbalanced:\n",
    "- Some families contain thousands of sequences\n",
    "- Many families contain only a few\n",
    "\n",
    "This automated selection process allows us to construct\n",
    "a controlled and reproducible benchmark rather than relying\n",
    "on arbitrary manual choices.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e19bc7b-3c4d-40a1-b5a2-9d2985e3ebef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanned PF00001..PF00200 | candidates so far: 122\n",
      "Scanned PF00001..PF00400 | candidates so far: 213\n",
      "Scanned PF00001..PF00600 | candidates so far: 297\n",
      "Candidate families found: 300\n",
      "First 10: ['PF00001', 'PF00004', 'PF00005', 'PF00006', 'PF00008', 'PF00009', 'PF00010', 'PF00011', 'PF00012', 'PF00013']\n"
     ]
    }
   ],
   "source": [
    "def build_query_for_pfam(pfam_id: str) -> str:\n",
    "    reviewed = \" AND (reviewed:true)\" if REVIEWED_ONLY else \"\"\n",
    "    pfam_clause = f\"(xref:pfam-{pfam_id})\"\n",
    "    return (\n",
    "        f\"{pfam_clause}\"\n",
    "        f\" AND (fragment:false)\"\n",
    "        f\"{reviewed}\"\n",
    "        f\" AND (length:[{LENGTH_MIN} TO {LENGTH_MAX}])\"\n",
    "    )\n",
    "\n",
    "# Discover candidates with enough sequences\n",
    "selected = []\n",
    "for i in range(1, SCAN_PFAM_MAX + 1):\n",
    "    pf = f\"PF{i:05d}\"\n",
    "    c = uniprot_count(build_query_for_pfam(pf))\n",
    "    if c >= PER_CLASS:\n",
    "        selected.append(pf)\n",
    "\n",
    "    # buffer so we don't lose too many families during cleaning\n",
    "    if len(selected) >= int(TARGET_FAMILIES * 1.5):\n",
    "        break\n",
    "\n",
    "    if i % 200 == 0:\n",
    "        print(f\"Scanned PF00001..PF{i:05d} | candidates so far: {len(selected)}\")\n",
    "        time.sleep(0.05)\n",
    "\n",
    "print(\"Candidate families found:\", len(selected))\n",
    "print(\"First 10:\", selected[:10])\n",
    "\n",
    "assert len(selected) >= TARGET_FAMILIES, (\n",
    "    f\"Not enough families found. Found {len(selected)} candidates, need {TARGET_FAMILIES}. \"\n",
    "    f\"Try: set REVIEWED_ONLY=False, lower PER_CLASS, increase SCAN_PFAM_MAX.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1011d12f-98d2-4db3-a21f-25154a9a41cb",
   "metadata": {},
   "source": [
    "## Build Dataset (Part A) : Standardize UniProt Column Names\n",
    "\n",
    "UniProt can return slightly different column headers depending on the endpoint or fields.\n",
    "For example, the accession column may appear as `Entry` or `Accession`.\n",
    "\n",
    "To keep the downstream code consistent, we normalize the DataFrame so that it always has:\n",
    "\n",
    "- `accession`\n",
    "- `sequence`\n",
    "- `protein_name`\n",
    "- `organism_name`\n",
    "- `length`\n",
    "\n",
    "This prevents bugs when we scale up to many families.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e32d15cb-2c7f-4410-8516-ada03ff1bfbb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _normalize_uniprot_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Map lowercase column names -> original column names (case-insensitive matching)\n",
    "    cmap = {c.lower(): c for c in df.columns}\n",
    "\n",
    "    # Try multiple possible names UniProt might use\n",
    "    acc = cmap.get(\"accession\") or cmap.get(\"entry\")\n",
    "    seq = cmap.get(\"sequence\")\n",
    "    prot = cmap.get(\"protein_name\") or cmap.get(\"protein names\")\n",
    "    org  = cmap.get(\"organism_name\") or cmap.get(\"organism\")\n",
    "    leng = cmap.get(\"length\")\n",
    "\n",
    "    # accession + sequence are mandatory; if missing, stop early with a clear error\n",
    "    if acc is None or seq is None:\n",
    "        raise ValueError(f\"Missing accession/sequence. Columns: {list(df.columns)}\")\n",
    "\n",
    "    # Rename to our standard schema\n",
    "    return df.rename(columns={\n",
    "        acc: \"accession\",\n",
    "        seq: \"sequence\",\n",
    "        (prot or \"protein_name\"): \"protein_name\",\n",
    "        (org  or \"organism_name\"): \"organism_name\",\n",
    "        (leng or \"length\"): \"length\",\n",
    "    }).copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cedc94eb-4773-4412-8ae3-71a0698785e5",
   "metadata": {},
   "source": [
    "## Build Dataset (Part B) : Fetch One Family (with Caching)\n",
    "\n",
    "For each Pfam family:\n",
    "1. Build the UniProt query\n",
    "2. Download up to `n` sequences\n",
    "3. Clean and attach the family label\n",
    "4. Cache the result locally (TSV file)\n",
    "\n",
    "Caching is important because:\n",
    "- It avoids repeating API requests\n",
    "- It speeds up reruns\n",
    "- It makes the notebook reproducible\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a07e5aa5-ac2d-43da-bf38-407b1df6c4c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_family_df(label: str, pfam_id: str, n: int) -> pd.DataFrame:\n",
    "    # Cache path so we don't re-download the same data every time\n",
    "    cache_path = os.path.join(\n",
    "        cfg.cache_dir, f\"{label}_{pfam_id}_n{n}_rev{int(REVIEWED_ONLY)}.tsv\"\n",
    "    )\n",
    "\n",
    "    # If cached file exists, load it immediately\n",
    "    if os.path.exists(cache_path):\n",
    "        return pd.read_csv(cache_path, sep=\"\\t\")\n",
    "\n",
    "    # Build query and fetch from UniProt\n",
    "    query = build_query_for_pfam(pfam_id)\n",
    "    fields = \"accession,protein_name,organism_name,length,sequence\"\n",
    "    df = fetch_uniprot_tsv(query=query, fields=fields, size=500, max_rows=n)\n",
    "\n",
    "    # If UniProt returns nothing, return empty DataFrame\n",
    "    if df.empty:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Normalize columns and basic cleaning\n",
    "    df = _normalize_uniprot_columns(df)\n",
    "    df = df.dropna(subset=[\"sequence\"]).copy()\n",
    "\n",
    "    # Add label column (ground-truth family)\n",
    "    df[\"label\"] = label\n",
    "\n",
    "    # Save to cache\n",
    "    df.to_csv(cache_path, sep=\"\\t\", index=False)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4090f75f-be9f-4fec-b030-173dfc509892",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Build Dataset (Part C): Collect Many Families into One Dataset\n",
    "\n",
    "We now:\n",
    "- Select the first `TARGET_FAMILIES` Pfams from our candidate list\n",
    "- Create a dictionary mapping each family label to its Pfam ID\n",
    "- Fetch sequences for each family and combine them into one dataset (`data`)\n",
    "\n",
    "At the end, `data` contains:\n",
    "- sequences\n",
    "- metadata\n",
    "- a family label for supervised learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "663d7cbc-acc3-49a3-a890-475ef412d7ff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PFAM_PF00001 PF00001 rows: 150\n",
      "PFAM_PF00004 PF00004 rows: 150\n",
      "PFAM_PF00005 PF00005 rows: 150\n",
      "PFAM_PF00006 PF00006 rows: 150\n",
      "PFAM_PF00008 PF00008 rows: 150\n",
      "PFAM_PF00009 PF00009 rows: 150\n",
      "PFAM_PF00010 PF00010 rows: 150\n",
      "PFAM_PF00011 PF00011 rows: 150\n",
      "PFAM_PF00012 PF00012 rows: 150\n",
      "PFAM_PF00013 PF00013 rows: 150\n",
      "PFAM_PF00014 PF00014 rows: 150\n",
      "PFAM_PF00016 PF00016 rows: 150\n",
      "PFAM_PF00017 PF00017 rows: 150\n",
      "PFAM_PF00018 PF00018 rows: 150\n",
      "PFAM_PF00019 PF00019 rows: 150\n",
      "PFAM_PF00022 PF00022 rows: 150\n",
      "PFAM_PF00023 PF00023 rows: 150\n",
      "PFAM_PF00025 PF00025 rows: 150\n",
      "PFAM_PF00026 PF00026 rows: 150\n",
      "PFAM_PF00027 PF00027 rows: 150\n",
      "PFAM_PF00028 PF00028 rows: 150\n",
      "PFAM_PF00032 PF00032 rows: 150\n",
      "PFAM_PF00033 PF00033 rows: 150\n",
      "PFAM_PF00034 PF00034 rows: 150\n",
      "PFAM_PF00035 PF00035 rows: 150\n",
      "PFAM_PF00038 PF00038 rows: 150\n",
      "PFAM_PF00041 PF00041 rows: 150\n",
      "PFAM_PF00042 PF00042 rows: 150\n",
      "PFAM_PF00043 PF00043 rows: 150\n",
      "PFAM_PF00044 PF00044 rows: 150\n",
      "PFAM_PF00046 PF00046 rows: 150\n",
      "PFAM_PF00047 PF00047 rows: 150\n",
      "PFAM_PF00048 PF00048 rows: 150\n",
      "PFAM_PF00049 PF00049 rows: 150\n",
      "PFAM_PF00056 PF00056 rows: 150\n",
      "PFAM_PF00059 PF00059 rows: 150\n",
      "PFAM_PF00061 PF00061 rows: 150\n",
      "PFAM_PF00067 PF00067 rows: 150\n",
      "PFAM_PF00068 PF00068 rows: 150\n",
      "PFAM_PF00069 PF00069 rows: 150\n",
      "PFAM_PF00071 PF00071 rows: 150\n",
      "PFAM_PF00072 PF00072 rows: 150\n",
      "PFAM_PF00074 PF00074 rows: 150\n",
      "PFAM_PF00075 PF00075 rows: 150\n",
      "PFAM_PF00076 PF00076 rows: 150\n",
      "PFAM_PF00079 PF00079 rows: 150\n",
      "PFAM_PF00080 PF00080 rows: 150\n",
      "PFAM_PF00081 PF00081 rows: 150\n",
      "PFAM_PF00082 PF00082 rows: 150\n",
      "PFAM_PF00083 PF00083 rows: 150\n",
      "PFAM_PF00084 PF00084 rows: 150\n",
      "PFAM_PF00085 PF00085 rows: 150\n",
      "PFAM_PF00089 PF00089 rows: 150\n",
      "PFAM_PF00091 PF00091 rows: 150\n",
      "PFAM_PF00096 PF00096 rows: 150\n",
      "PFAM_PF00097 PF00097 rows: 150\n",
      "PFAM_PF00098 PF00098 rows: 150\n",
      "PFAM_PF00101 PF00101 rows: 150\n",
      "PFAM_PF00103 PF00103 rows: 150\n",
      "PFAM_PF00104 PF00104 rows: 150\n",
      "PFAM_PF00105 PF00105 rows: 150\n",
      "PFAM_PF00106 PF00106 rows: 150\n",
      "PFAM_PF00107 PF00107 rows: 150\n",
      "PFAM_PF00108 PF00108 rows: 150\n",
      "PFAM_PF00111 PF00111 rows: 150\n",
      "PFAM_PF00112 PF00112 rows: 150\n",
      "PFAM_PF00113 PF00113 rows: 150\n",
      "PFAM_PF00115 PF00115 rows: 150\n",
      "PFAM_PF00116 PF00116 rows: 150\n",
      "PFAM_PF00117 PF00117 rows: 150\n",
      "PFAM_PF00118 PF00118 rows: 150\n",
      "PFAM_PF00119 PF00119 rows: 150\n",
      "PFAM_PF00120 PF00120 rows: 150\n",
      "PFAM_PF00121 PF00121 rows: 150\n",
      "PFAM_PF00122 PF00122 rows: 150\n",
      "PFAM_PF00124 PF00124 rows: 150\n",
      "PFAM_PF00125 PF00125 rows: 150\n",
      "PFAM_PF00126 PF00126 rows: 150\n",
      "PFAM_PF00128 PF00128 rows: 150\n",
      "PFAM_PF00130 PF00130 rows: 150\n",
      "PFAM_PF00132 PF00132 rows: 150\n",
      "PFAM_PF00133 PF00133 rows: 150\n",
      "PFAM_PF00134 PF00134 rows: 150\n",
      "PFAM_PF00137 PF00137 rows: 150\n",
      "PFAM_PF00139 PF00139 rows: 150\n",
      "PFAM_PF00141 PF00141 rows: 150\n",
      "PFAM_PF00142 PF00142 rows: 150\n",
      "PFAM_PF00146 PF00146 rows: 150\n",
      "PFAM_PF00148 PF00148 rows: 150\n",
      "PFAM_PF00149 PF00149 rows: 150\n",
      "PFAM_PF00152 PF00152 rows: 150\n",
      "PFAM_PF00153 PF00153 rows: 150\n",
      "PFAM_PF00154 PF00154 rows: 150\n",
      "PFAM_PF00155 PF00155 rows: 150\n",
      "PFAM_PF00156 PF00156 rows: 150\n",
      "PFAM_PF00158 PF00158 rows: 150\n",
      "PFAM_PF00160 PF00160 rows: 150\n",
      "PFAM_PF00162 PF00162 rows: 150\n",
      "PFAM_PF00163 PF00163 rows: 150\n",
      "PFAM_PF00164 PF00164 rows: 150\n",
      "PFAM_PF00166 PF00166 rows: 150\n",
      "PFAM_PF00168 PF00168 rows: 150\n",
      "PFAM_PF00169 PF00169 rows: 150\n",
      "PFAM_PF00170 PF00170 rows: 150\n",
      "PFAM_PF00171 PF00171 rows: 150\n",
      "PFAM_PF00172 PF00172 rows: 150\n",
      "PFAM_PF00173 PF00173 rows: 150\n",
      "PFAM_PF00175 PF00175 rows: 150\n",
      "PFAM_PF00176 PF00176 rows: 150\n",
      "PFAM_PF00177 PF00177 rows: 150\n",
      "PFAM_PF00179 PF00179 rows: 150\n",
      "PFAM_PF00180 PF00180 rows: 150\n",
      "PFAM_PF00181 PF00181 rows: 150\n",
      "PFAM_PF00183 PF00183 rows: 150\n",
      "PFAM_PF00185 PF00185 rows: 150\n",
      "PFAM_PF00188 PF00188 rows: 150\n",
      "PFAM_PF00189 PF00189 rows: 150\n",
      "PFAM_PF00190 PF00190 rows: 150\n",
      "PFAM_PF00195 PF00195 rows: 150\n",
      "PFAM_PF00196 PF00196 rows: 150\n",
      "PFAM_PF00199 PF00199 rows: 150\n",
      "PFAM_PF00200 PF00200 rows: 150\n",
      "PFAM_PF00201 PF00201 rows: 150\n",
      "PFAM_PF00202 PF00202 rows: 150\n",
      "PFAM_PF00203 PF00203 rows: 150\n",
      "PFAM_PF00205 PF00205 rows: 150\n",
      "PFAM_PF00206 PF00206 rows: 150\n",
      "PFAM_PF00210 PF00210 rows: 150\n",
      "PFAM_PF00213 PF00213 rows: 150\n",
      "PFAM_PF00215 PF00215 rows: 150\n",
      "PFAM_PF00216 PF00216 rows: 150\n",
      "PFAM_PF00218 PF00218 rows: 150\n",
      "PFAM_PF00221 PF00221 rows: 150\n",
      "PFAM_PF00223 PF00223 rows: 150\n",
      "PFAM_PF00225 PF00225 rows: 150\n",
      "PFAM_PF00226 PF00226 rows: 150\n",
      "PFAM_PF00227 PF00227 rows: 150\n",
      "PFAM_PF00230 PF00230 rows: 150\n",
      "PFAM_PF00231 PF00231 rows: 150\n",
      "PFAM_PF00232 PF00232 rows: 150\n",
      "PFAM_PF00234 PF00234 rows: 150\n",
      "PFAM_PF00235 PF00235 rows: 150\n",
      "PFAM_PF00237 PF00237 rows: 150\n",
      "PFAM_PF00238 PF00238 rows: 150\n",
      "PFAM_PF00240 PF00240 rows: 150\n",
      "PFAM_PF00248 PF00248 rows: 150\n",
      "PFAM_PF00249 PF00249 rows: 150\n",
      "PFAM_PF00250 PF00250 rows: 150\n",
      "PFAM_PF00252 PF00252 rows: 150\n",
      "PFAM_PF00253 PF00253 rows: 150\n",
      "PFAM_PF00254 PF00254 rows: 150\n",
      "PFAM_PF00258 PF00258 rows: 150\n",
      "PFAM_PF00265 PF00265 rows: 150\n",
      "PFAM_PF00266 PF00266 rows: 150\n",
      "PFAM_PF00270 PF00270 rows: 150\n",
      "PFAM_PF00271 PF00271 rows: 150\n",
      "PFAM_PF00275 PF00275 rows: 150\n",
      "PFAM_PF00276 PF00276 rows: 150\n",
      "PFAM_PF00281 PF00281 rows: 150\n",
      "PFAM_PF00282 PF00282 rows: 150\n",
      "PFAM_PF00288 PF00288 rows: 150\n",
      "PFAM_PF00290 PF00290 rows: 150\n",
      "PFAM_PF00291 PF00291 rows: 150\n",
      "PFAM_PF00293 PF00293 rows: 150\n",
      "PFAM_PF00294 PF00294 rows: 150\n",
      "PFAM_PF00295 PF00295 rows: 150\n",
      "PFAM_PF00296 PF00296 rows: 150\n",
      "PFAM_PF00297 PF00297 rows: 150\n",
      "PFAM_PF00298 PF00298 rows: 150\n",
      "PFAM_PF00300 PF00300 rows: 150\n",
      "PFAM_PF00303 PF00303 rows: 150\n",
      "PFAM_PF00306 PF00306 rows: 150\n",
      "PFAM_PF00307 PF00307 rows: 150\n",
      "PFAM_PF00308 PF00308 rows: 150\n",
      "PFAM_PF00311 PF00311 rows: 150\n",
      "PFAM_PF00312 PF00312 rows: 150\n",
      "PFAM_PF00313 PF00313 rows: 150\n",
      "PFAM_PF00316 PF00316 rows: 150\n",
      "PFAM_PF00318 PF00318 rows: 150\n",
      "PFAM_PF00319 PF00319 rows: 150\n",
      "PFAM_PF00320 PF00320 rows: 150\n",
      "PFAM_PF00326 PF00326 rows: 150\n",
      "PFAM_PF00327 PF00327 rows: 150\n",
      "PFAM_PF00329 PF00329 rows: 150\n",
      "PFAM_PF00330 PF00330 rows: 150\n",
      "PFAM_PF00333 PF00333 rows: 150\n",
      "PFAM_PF00334 PF00334 rows: 150\n",
      "PFAM_PF00335 PF00335 rows: 150\n",
      "PFAM_PF00338 PF00338 rows: 150\n",
      "PFAM_PF00342 PF00342 rows: 150\n",
      "PFAM_PF00344 PF00344 rows: 150\n",
      "PFAM_PF00346 PF00346 rows: 150\n",
      "PFAM_PF00347 PF00347 rows: 150\n",
      "PFAM_PF00348 PF00348 rows: 150\n",
      "PFAM_PF00355 PF00355 rows: 150\n",
      "PFAM_PF00356 PF00356 rows: 150\n",
      "PFAM_PF00361 PF00361 rows: 150\n",
      "PFAM_PF00364 PF00364 rows: 150\n",
      "PFAM_PF00365 PF00365 rows: 150\n",
      "PFAM_PF00366 PF00366 rows: 150\n",
      "\n",
      "Raw rows: 30000 Raw classes: 200\n"
     ]
    }
   ],
   "source": [
    "pfams = selected[:TARGET_FAMILIES]\n",
    "FAMILIES = {f\"PFAM_{pf}\": pf for pf in pfams}\n",
    "\n",
    "dfs = []\n",
    "for label, pf in FAMILIES.items():\n",
    "    df_i = get_family_df(label, pf, PER_CLASS)\n",
    "    if not df_i.empty:\n",
    "        dfs.append(df_i)\n",
    "    print(label, pf, \"rows:\", len(df_i))\n",
    "\n",
    "data = pd.concat(dfs, ignore_index=True)\n",
    "print(\"\\nRaw rows:\", len(data), \"Raw classes:\", data[\"label\"].nunique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b3b338-8103-49de-98fc-1a15ec2a84ef",
   "metadata": {},
   "source": [
    "## Build Dataset (Part D): Remove Duplicate Sequences\n",
    "\n",
    "The same protein sequence can sometimes appear multiple times\n",
    "(e.g., redundant entries across organisms or isoforms).\n",
    "\n",
    "We remove duplicate sequences to:\n",
    "- avoid leakage\n",
    "- prevent the classifier from memorizing identical sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68757140-d732-488e-a141-979582d928bb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After de-dup sequences: 27065\n"
     ]
    }
   ],
   "source": [
    "data = data.drop_duplicates(subset=[\"sequence\"]).reset_index(drop=True)\n",
    "print(\"After de-dup sequences:\", len(data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fb9158-d94c-430b-82cd-939d3378bb94",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Build Dataset (Part E): Remove Multi-Label Accessions\n",
    "\n",
    "A protein can sometimes contain multiple domains and therefore match multiple Pfam families.\n",
    "If we allow those proteins, the same sequence could appear with different labels,\n",
    "which breaks the assumption of single-label classification.\n",
    "\n",
    "To keep the task clean, we remove accessions that map to more than one selected family.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1cccd768-14d8-491f-927e-fe0d9eca14de",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After removing multi-label accessions: 27065\n"
     ]
    }
   ],
   "source": [
    "acc_n = data.groupby(\"accession\")[\"label\"].nunique()\n",
    "data = data[data[\"accession\"].isin(acc_n[acc_n == 1].index)].reset_index(drop=True)\n",
    "print(\"After removing multi-label accessions:\", len(data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7f2e80-2c25-4da7-a044-00464c0e194c",
   "metadata": {},
   "source": [
    "## Build Dataset (Part F): Drop Families That Became Too Small\n",
    "\n",
    "After cleaning, some families may lose many proteins.\n",
    "To keep the benchmark stable, we drop families with fewer than `MIN_AFTER_CLEAN` sequences.\n",
    "\n",
    "This ensures each class has enough training examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a4ebb95a-2740-45a8-aeef-06227f09f9b0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final rows: 27026\n",
      "Final classes: 197\n",
      "Per-class min/median/max: 82 141 150\n"
     ]
    }
   ],
   "source": [
    "vc = data[\"label\"].value_counts()\n",
    "keep = vc[vc >= MIN_AFTER_CLEAN].index\n",
    "data = data[data[\"label\"].isin(keep)].reset_index(drop=True)\n",
    "\n",
    "print(\"Final rows:\", len(data))\n",
    "print(\"Final classes:\", data[\"label\"].nunique())\n",
    "print(\"Per-class min/median/max:\",\n",
    "      data[\"label\"].value_counts().min(),\n",
    "      int(data[\"label\"].value_counts().median()),\n",
    "      data[\"label\"].value_counts().max())\n",
    "\n",
    "assert data[\"label\"].nunique() >= int(0.9 * TARGET_FAMILIES), (\n",
    "    \"Too many families dropped after cleaning. Try lowering MIN_AFTER_CLEAN or PER_CLASS, \"\n",
    "    \"or set REVIEWED_ONLY=False to get more sequences.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f50845-6273-4bd6-84d6-a5eea63010c3",
   "metadata": {},
   "source": [
    "## Generating Protein Embeddings Using ESM-2\n",
    "\n",
    "In this step, we convert raw protein sequences into numerical representations\n",
    "using a pretrained ESM-2 transformer model.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Loading the Pretrained Model\n",
    "\n",
    "We load two components:\n",
    "\n",
    "- **Tokenizer** → Converts amino acid sequences into token IDs\n",
    "- **Model** → Transformer architecture that produces contextual embeddings\n",
    "\n",
    "The model is moved to either:\n",
    "- GPU (if available), or\n",
    "- CPU\n",
    "\n",
    "We set the model to evaluation mode (`model.eval()`) since we are not fine-tuning it.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. How Embedding Works\n",
    "\n",
    "Each protein sequence is:\n",
    "\n",
    "1. Truncated to a maximum length (`MAX_LEN`) to control memory usage.\n",
    "2. Tokenized into integer IDs.\n",
    "3. Passed through the transformer model.\n",
    "4. Converted into contextual token embeddings.\n",
    "\n",
    "The output of ESM-2 is:\n",
    "\n",
    "Each amino acid receives a contextual embedding that depends on the entire sequence.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Mean Pooling\n",
    "\n",
    "Since proteins have variable lengths, we convert token-level embeddings\n",
    "into a fixed-length vector using mean pooling:\n",
    "\n",
    "- Multiply embeddings by attention mask (to ignore padding)\n",
    "- Sum across sequence dimension\n",
    "- Divide by number of valid tokens\n",
    "\n",
    "This produces one vector per protein:\n",
    "This vector is the **protein embedding**.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Embedding the Full Dataset\n",
    "\n",
    "We process the dataset in mini-batches to:\n",
    "\n",
    "- Reduce memory usage\n",
    "- Improve GPU efficiency\n",
    "- Enable scaling to large datasets\n",
    "\n",
    "The final result:\n",
    "\n",
    "- `X` → matrix of protein embeddings (one row per protein)\n",
    "- `y` → corresponding Pfam family labels\n",
    "\n",
    "---\n",
    "\n",
    "### 5. What Does `X` Represent?\n",
    "\n",
    "If the embedding dimension is `H`, then:\n",
    "\n",
    "\n",
    "Each row represents a protein in a high-dimensional embedding space\n",
    "learned from millions of protein sequences.\n",
    "\n",
    "If ESM-2 has captured biological structure correctly,\n",
    "proteins from the same family should cluster together in this space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dbfe075a-5cf4-4942-9f4e-10c17716ddab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding with: facebook/esm2_t30_150M_UR50D\n",
      "Device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping import of cpp extensions due to incompatible torch version 2.9.1+cu128 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info\n",
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t30_150M_UR50D and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Embedding: 100%|██████████| 6757/6757 [10:17<00:00, 10.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (27026, 640)\n",
      "Classes: 197\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "assert \"data\" in globals() and not data.empty\n",
    "\n",
    "DEVICE = cfg.device\n",
    "MODEL_NAME = ESM_MODEL_NAME\n",
    "\n",
    "print(\"Embedding with:\", MODEL_NAME)\n",
    "print(\"Device:\", DEVICE)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME).to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "@torch.no_grad()\n",
    "def embed_batch(seqs):\n",
    "    seqs = [str(s)[:MAX_LEN] for s in seqs]\n",
    "    toks = tokenizer(seqs, return_tensors=\"pt\", padding=True, truncation=True, max_length=MAX_LEN)\n",
    "    toks = {k: v.to(DEVICE) for k, v in toks.items()}\n",
    "    out = model(**toks).last_hidden_state          # [B, T, H]\n",
    "    mask = toks[\"attention_mask\"].unsqueeze(-1)    # [B, T, 1]\n",
    "    pooled = (out * mask).sum(dim=1) / mask.sum(dim=1).clamp(min=1)\n",
    "    return pooled.float().cpu().numpy()\n",
    "\n",
    "def embed_dataset(df, batch_size=BATCH_SIZE):\n",
    "    seqs = df[\"sequence\"].astype(str).tolist()\n",
    "    embs = []\n",
    "    for i in tqdm(range(0, len(seqs), batch_size), desc=\"Embedding\"):\n",
    "        embs.append(embed_batch(seqs[i:i+batch_size]))\n",
    "    return np.vstack(embs)\n",
    "\n",
    "X = embed_dataset(data)\n",
    "y = data[\"label\"].values\n",
    "\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"Classes:\", len(np.unique(y)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24968ce-79a6-4593-8d87-2b81d61a4c13",
   "metadata": {},
   "source": [
    "## Training a Linear Classifier on ESM-2 Embeddings\n",
    "\n",
    "In this section, we evaluate whether ESM-2 embeddings encode\n",
    "protein family information in a linearly separable way.\n",
    "\n",
    "If a simple linear model achieves high accuracy,\n",
    "it suggests that the embedding space already captures\n",
    "functional structure.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Encoding Labels\n",
    "\n",
    "Protein family labels are categorical (strings).\n",
    "Machine learning models require numeric labels.\n",
    "\n",
    "We use `LabelEncoder` to convert:\n",
    "\"PFAM_PF00001\" → 0\n",
    "\"PFAM_PF00004\" → 1\n",
    "...\n",
    "\n",
    "This creates integer class indices for training.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Train-Test Split\n",
    "\n",
    "We split the dataset into:\n",
    "\n",
    "- 75% training data\n",
    "- 25% test data\n",
    "\n",
    "We use **stratified sampling** to preserve class balance\n",
    "across both sets.\n",
    "\n",
    "This ensures fair evaluation.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Standardizing Embeddings (Very Important)\n",
    "\n",
    "Even though embeddings are learned representations,\n",
    "their dimensions may have different scales.\n",
    "\n",
    "We standardize features using: X_standardized = (X - mean) / std\n",
    "\n",
    "Important:\n",
    "- We compute mean and standard deviation **only from training data**\n",
    "- We apply the same transformation to test data\n",
    "\n",
    "This avoids data leakage and improves optimization stability.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Moving Data to GPU\n",
    "\n",
    "We convert NumPy arrays into PyTorch tensors and move them to:\n",
    "\n",
    "- GPU (if available), or\n",
    "- CPU\n",
    "\n",
    "This allows us to train efficiently at scale.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Mini-Batch Training\n",
    "\n",
    "Instead of training on the entire dataset at once,\n",
    "we use mini-batches (size 256).\n",
    "\n",
    "Advantages:\n",
    "- More stable optimization\n",
    "- Faster convergence\n",
    "- Better GPU utilization\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Linear Classifier\n",
    "\n",
    "We define a simple linear layer: Linear(in_dim → num_classes)\n",
    "\n",
    "This performs: logits = W·x + b\n",
    "\n",
    "No hidden layers.\n",
    "\n",
    "This is intentionally simple —\n",
    "we are testing the quality of embeddings,\n",
    "not building a deep classifier.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Loss Function\n",
    "\n",
    "We use **CrossEntropyLoss**, which:\n",
    "\n",
    "- Combines softmax + log likelihood\n",
    "- Is standard for multi-class classification\n",
    "\n",
    "---\n",
    "\n",
    "### 8. Optimization\n",
    "\n",
    "We use SGD with:\n",
    "- Learning rate = 0.2\n",
    "- Momentum = 0.9\n",
    "- Weight decay for regularization\n",
    "\n",
    "We train for 30 epochs.\n",
    "\n",
    "---\n",
    "\n",
    "### 9. Evaluation\n",
    "\n",
    "After training, we:\n",
    "\n",
    "- Switch to evaluation mode\n",
    "- Compute predictions on test data\n",
    "- Report accuracy and class-level precision/recall/F1\n",
    "\n",
    "---\n",
    "\n",
    "## What Are We Testing?\n",
    "\n",
    "If the classifier achieves high accuracy,\n",
    "it means:\n",
    "\n",
    "- Protein families are linearly separable\n",
    "- ESM-2 embeddings encode functional similarity\n",
    "- No fine-tuning was required\n",
    "\n",
    "This validates the representational power of the pretrained model.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b4b05032-0320-4c2e-8de6-f086574eb77c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 01 | loss 0.6798\n",
      "epoch 05 | loss 0.0399\n",
      "epoch 10 | loss 0.0232\n",
      "epoch 15 | loss 0.0217\n",
      "epoch 20 | loss 0.0179\n",
      "epoch 25 | loss 0.0162\n",
      "epoch 30 | loss 0.0155\n",
      "Accuracy: 0.9758768684327365\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "PFAM_PF00001      1.000     1.000     1.000        38\n",
      "PFAM_PF00004      0.921     0.946     0.933        37\n",
      "PFAM_PF00005      1.000     0.973     0.986        37\n",
      "PFAM_PF00006      0.714     0.946     0.814        37\n",
      "PFAM_PF00008      0.921     0.921     0.921        38\n",
      "PFAM_PF00009      1.000     0.970     0.985        33\n",
      "PFAM_PF00010      0.925     0.974     0.949        38\n",
      "PFAM_PF00011      1.000     0.972     0.986        36\n",
      "PFAM_PF00012      1.000     1.000     1.000        34\n",
      "PFAM_PF00013      1.000     0.943     0.971        35\n",
      "PFAM_PF00014      1.000     1.000     1.000        37\n",
      "PFAM_PF00016      1.000     1.000     1.000        36\n",
      "PFAM_PF00017      0.854     0.946     0.897        37\n",
      "PFAM_PF00018      0.818     0.692     0.750        26\n",
      "PFAM_PF00019      0.974     1.000     0.987        37\n",
      "PFAM_PF00022      1.000     1.000     1.000        26\n",
      "PFAM_PF00023      0.949     0.974     0.961        38\n",
      "PFAM_PF00025      0.941     0.970     0.955        33\n",
      "PFAM_PF00026      1.000     1.000     1.000        36\n",
      "PFAM_PF00027      1.000     0.973     0.986        37\n",
      "PFAM_PF00028      1.000     1.000     1.000        38\n",
      "PFAM_PF00032      1.000     0.947     0.973        38\n",
      "PFAM_PF00034      1.000     1.000     1.000        34\n",
      "PFAM_PF00035      1.000     0.912     0.954        34\n",
      "PFAM_PF00038      1.000     1.000     1.000        38\n",
      "PFAM_PF00041      0.889     0.865     0.877        37\n",
      "PFAM_PF00042      1.000     0.943     0.971        35\n",
      "PFAM_PF00043      1.000     0.974     0.987        38\n",
      "PFAM_PF00044      1.000     1.000     1.000        34\n",
      "PFAM_PF00046      1.000     0.974     0.987        38\n",
      "PFAM_PF00047      0.943     0.943     0.943        35\n",
      "PFAM_PF00048      1.000     1.000     1.000        37\n",
      "PFAM_PF00049      1.000     1.000     1.000        36\n",
      "PFAM_PF00056      1.000     1.000     1.000        36\n",
      "PFAM_PF00059      0.944     0.971     0.958        35\n",
      "PFAM_PF00061      1.000     1.000     1.000        35\n",
      "PFAM_PF00067      1.000     0.974     0.987        38\n",
      "PFAM_PF00068      1.000     1.000     1.000        37\n",
      "PFAM_PF00069      0.935     0.784     0.853        37\n",
      "PFAM_PF00071      1.000     0.944     0.971        36\n",
      "PFAM_PF00072      0.882     0.811     0.845        37\n",
      "PFAM_PF00074      1.000     1.000     1.000        37\n",
      "PFAM_PF00075      1.000     1.000     1.000        32\n",
      "PFAM_PF00076      0.791     0.919     0.850        37\n",
      "PFAM_PF00079      1.000     1.000     1.000        38\n",
      "PFAM_PF00080      1.000     1.000     1.000        35\n",
      "PFAM_PF00081      1.000     1.000     1.000        34\n",
      "PFAM_PF00082      0.974     1.000     0.987        37\n",
      "PFAM_PF00083      1.000     1.000     1.000        38\n",
      "PFAM_PF00084      0.929     0.867     0.897        30\n",
      "PFAM_PF00085      0.925     0.974     0.949        38\n",
      "PFAM_PF00089      1.000     0.903     0.949        31\n",
      "PFAM_PF00091      1.000     1.000     1.000        27\n",
      "PFAM_PF00096      1.000     1.000     1.000        38\n",
      "PFAM_PF00097      0.875     0.946     0.909        37\n",
      "PFAM_PF00098      0.969     0.838     0.899        37\n",
      "PFAM_PF00101      1.000     1.000     1.000        37\n",
      "PFAM_PF00103      1.000     1.000     1.000        37\n",
      "PFAM_PF00104      1.000     1.000     1.000        37\n",
      "PFAM_PF00106      1.000     1.000     1.000        37\n",
      "PFAM_PF00107      1.000     1.000     1.000        38\n",
      "PFAM_PF00108      1.000     1.000     1.000        36\n",
      "PFAM_PF00111      0.972     0.972     0.972        36\n",
      "PFAM_PF00112      0.974     1.000     0.987        37\n",
      "PFAM_PF00113      1.000     1.000     1.000        34\n",
      "PFAM_PF00115      1.000     1.000     1.000        36\n",
      "PFAM_PF00116      1.000     1.000     1.000        34\n",
      "PFAM_PF00117      0.946     1.000     0.972        35\n",
      "PFAM_PF00118      1.000     1.000     1.000        35\n",
      "PFAM_PF00119      1.000     1.000     1.000        36\n",
      "PFAM_PF00120      1.000     0.971     0.986        35\n",
      "PFAM_PF00121      0.971     1.000     0.986        34\n",
      "PFAM_PF00122      1.000     1.000     1.000        37\n",
      "PFAM_PF00124      0.935     1.000     0.967        29\n",
      "PFAM_PF00125      0.966     1.000     0.982        28\n",
      "PFAM_PF00126      1.000     0.970     0.985        33\n",
      "PFAM_PF00128      1.000     1.000     1.000        37\n",
      "PFAM_PF00130      0.811     0.968     0.882        31\n",
      "PFAM_PF00132      1.000     1.000     1.000        35\n",
      "PFAM_PF00133      1.000     1.000     1.000        36\n",
      "PFAM_PF00134      1.000     1.000     1.000        37\n",
      "PFAM_PF00137      1.000     1.000     1.000        30\n",
      "PFAM_PF00139      1.000     1.000     1.000        38\n",
      "PFAM_PF00141      1.000     1.000     1.000        37\n",
      "PFAM_PF00142      1.000     1.000     1.000        36\n",
      "PFAM_PF00146      1.000     1.000     1.000        36\n",
      "PFAM_PF00148      1.000     1.000     1.000        38\n",
      "PFAM_PF00149      1.000     1.000     1.000        34\n",
      "PFAM_PF00152      1.000     1.000     1.000        35\n",
      "PFAM_PF00153      1.000     1.000     1.000        38\n",
      "PFAM_PF00154      1.000     1.000     1.000        28\n",
      "PFAM_PF00155      1.000     1.000     1.000        37\n",
      "PFAM_PF00156      0.967     1.000     0.983        29\n",
      "PFAM_PF00158      0.917     1.000     0.957        33\n",
      "PFAM_PF00160      1.000     1.000     1.000        34\n",
      "PFAM_PF00162      1.000     0.971     0.985        34\n",
      "PFAM_PF00163      1.000     1.000     1.000        34\n",
      "PFAM_PF00164      1.000     1.000     1.000        31\n",
      "PFAM_PF00166      1.000     1.000     1.000        29\n",
      "PFAM_PF00168      0.943     0.971     0.957        34\n",
      "PFAM_PF00169      0.786     0.786     0.786        28\n",
      "PFAM_PF00170      1.000     1.000     1.000        37\n",
      "PFAM_PF00171      1.000     1.000     1.000        37\n",
      "PFAM_PF00172      1.000     0.892     0.943        37\n",
      "PFAM_PF00173      1.000     0.973     0.986        37\n",
      "PFAM_PF00175      0.667     0.640     0.653        25\n",
      "PFAM_PF00176      0.846     0.971     0.904        34\n",
      "PFAM_PF00177      0.935     1.000     0.967        29\n",
      "PFAM_PF00179      1.000     1.000     1.000        31\n",
      "PFAM_PF00180      0.974     1.000     0.987        37\n",
      "PFAM_PF00181      1.000     1.000     1.000        26\n",
      "PFAM_PF00183      1.000     1.000     1.000        36\n",
      "PFAM_PF00185      1.000     1.000     1.000        33\n",
      "PFAM_PF00188      1.000     0.973     0.986        37\n",
      "PFAM_PF00189      0.970     0.970     0.970        33\n",
      "PFAM_PF00190      1.000     1.000     1.000        37\n",
      "PFAM_PF00195      1.000     1.000     1.000        36\n",
      "PFAM_PF00196      0.862     1.000     0.926        25\n",
      "PFAM_PF00199      0.971     1.000     0.986        34\n",
      "PFAM_PF00200      0.974     1.000     0.987        38\n",
      "PFAM_PF00201      1.000     1.000     1.000        38\n",
      "PFAM_PF00202      0.971     1.000     0.985        33\n",
      "PFAM_PF00203      1.000     1.000     1.000        31\n",
      "PFAM_PF00205      1.000     1.000     1.000        34\n",
      "PFAM_PF00206      1.000     1.000     1.000        34\n",
      "PFAM_PF00210      1.000     1.000     1.000        28\n",
      "PFAM_PF00213      1.000     1.000     1.000        34\n",
      "PFAM_PF00215      1.000     0.970     0.985        33\n",
      "PFAM_PF00216      1.000     1.000     1.000        29\n",
      "PFAM_PF00218      1.000     0.970     0.985        33\n",
      "PFAM_PF00221      1.000     0.973     0.986        37\n",
      "PFAM_PF00223      1.000     1.000     1.000        34\n",
      "PFAM_PF00225      0.974     1.000     0.987        38\n",
      "PFAM_PF00226      1.000     0.944     0.971        36\n",
      "PFAM_PF00227      1.000     1.000     1.000        37\n",
      "PFAM_PF00230      1.000     0.947     0.973        38\n",
      "PFAM_PF00231      1.000     1.000     1.000        34\n",
      "PFAM_PF00232      1.000     1.000     1.000        37\n",
      "PFAM_PF00234      1.000     1.000     1.000        37\n",
      "PFAM_PF00235      1.000     1.000     1.000        32\n",
      "PFAM_PF00237      1.000     1.000     1.000        33\n",
      "PFAM_PF00238      1.000     1.000     1.000        32\n",
      "PFAM_PF00240      0.970     0.941     0.955        34\n",
      "PFAM_PF00248      0.974     1.000     0.987        37\n",
      "PFAM_PF00249      0.944     0.944     0.944        36\n",
      "PFAM_PF00250      0.974     1.000     0.987        38\n",
      "PFAM_PF00252      1.000     0.938     0.968        32\n",
      "PFAM_PF00253      0.968     0.938     0.952        32\n",
      "PFAM_PF00254      1.000     1.000     1.000        37\n",
      "PFAM_PF00258      0.682     0.682     0.682        22\n",
      "PFAM_PF00265      1.000     1.000     1.000        32\n",
      "PFAM_PF00266      1.000     1.000     1.000        36\n",
      "PFAM_PF00270      0.923     1.000     0.960        36\n",
      "PFAM_PF00275      1.000     1.000     1.000        35\n",
      "PFAM_PF00276      1.000     1.000     1.000        33\n",
      "PFAM_PF00281      1.000     0.939     0.969        33\n",
      "PFAM_PF00282      1.000     1.000     1.000        37\n",
      "PFAM_PF00288      1.000     1.000     1.000        36\n",
      "PFAM_PF00290      1.000     1.000     1.000        35\n",
      "PFAM_PF00291      1.000     1.000     1.000        35\n",
      "PFAM_PF00293      1.000     1.000     1.000        35\n",
      "PFAM_PF00294      1.000     1.000     1.000        33\n",
      "PFAM_PF00295      1.000     1.000     1.000        34\n",
      "PFAM_PF00296      1.000     1.000     1.000        33\n",
      "PFAM_PF00297      1.000     1.000     1.000        35\n",
      "PFAM_PF00298      1.000     1.000     1.000        30\n",
      "PFAM_PF00300      1.000     0.971     0.986        35\n",
      "PFAM_PF00303      0.971     0.971     0.971        35\n",
      "PFAM_PF00306      0.778     0.333     0.467        21\n",
      "PFAM_PF00307      0.921     1.000     0.959        35\n",
      "PFAM_PF00308      1.000     0.969     0.984        32\n",
      "PFAM_PF00311      0.970     1.000     0.985        32\n",
      "PFAM_PF00312      1.000     1.000     1.000        32\n",
      "PFAM_PF00313      1.000     1.000     1.000        23\n",
      "PFAM_PF00316      1.000     1.000     1.000        32\n",
      "PFAM_PF00318      1.000     1.000     1.000        34\n",
      "PFAM_PF00319      0.914     0.865     0.889        37\n",
      "PFAM_PF00320      0.846     0.917     0.880        36\n",
      "PFAM_PF00326      1.000     1.000     1.000        36\n",
      "PFAM_PF00327      1.000     1.000     1.000        33\n",
      "PFAM_PF00329      0.939     1.000     0.969        31\n",
      "PFAM_PF00330      1.000     1.000     1.000        36\n",
      "PFAM_PF00333      1.000     1.000     1.000        34\n",
      "PFAM_PF00334      0.966     0.875     0.918        32\n",
      "PFAM_PF00335      1.000     1.000     1.000        36\n",
      "PFAM_PF00338      0.968     1.000     0.984        30\n",
      "PFAM_PF00342      1.000     1.000     1.000        36\n",
      "PFAM_PF00344      1.000     1.000     1.000        32\n",
      "PFAM_PF00346      1.000     1.000     1.000        20\n",
      "PFAM_PF00347      1.000     1.000     1.000        33\n",
      "PFAM_PF00348      1.000     1.000     1.000        37\n",
      "PFAM_PF00355      0.974     1.000     0.987        37\n",
      "PFAM_PF00356      1.000     1.000     1.000        25\n",
      "PFAM_PF00361      1.000     1.000     1.000        35\n",
      "PFAM_PF00364      1.000     1.000     1.000        35\n",
      "PFAM_PF00365      1.000     1.000     1.000        32\n",
      "PFAM_PF00366      1.000     0.968     0.984        31\n",
      "\n",
      "    accuracy                          0.976      6757\n",
      "   macro avg      0.975     0.973     0.973      6757\n",
      "weighted avg      0.977     0.976     0.976      6757\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# --- Encode labels ---\n",
    "le = LabelEncoder()\n",
    "y_int = le.fit_transform(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_int, test_size=0.25, random_state=42, stratify=y_int\n",
    ")\n",
    "\n",
    "# --- Standardize using train stats only (VERY IMPORTANT) ---\n",
    "mu = X_train.mean(axis=0, keepdims=True)\n",
    "std = X_train.std(axis=0, keepdims=True) + 1e-6\n",
    "X_train_s = (X_train - mu) / std\n",
    "X_test_s  = (X_test  - mu) / std\n",
    "\n",
    "device = cfg.device\n",
    "num_classes = len(le.classes_)\n",
    "in_dim = X_train_s.shape[1]\n",
    "\n",
    "# --- Tensors on GPU ---\n",
    "X_train_t = torch.tensor(X_train_s, dtype=torch.float32, device=device)\n",
    "y_train_t = torch.tensor(y_train,   dtype=torch.long,   device=device)\n",
    "X_test_t  = torch.tensor(X_test_s,  dtype=torch.float32, device=device)\n",
    "y_test_t  = torch.tensor(y_test,    dtype=torch.long,   device=device)\n",
    "\n",
    "# --- DataLoader (mini-batches) ---\n",
    "ds = TensorDataset(X_train_t, y_train_t)\n",
    "loader = DataLoader(ds, batch_size=256, shuffle=True)\n",
    "\n",
    "# --- Linear classifier ---\n",
    "clf = torch.nn.Linear(in_dim, num_classes).to(device)\n",
    "\n",
    "# Try SGD for linear models (often better behaved than Adam here)\n",
    "opt = torch.optim.SGD(clf.parameters(), lr=0.2, momentum=0.9, weight_decay=1e-4)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# --- Train ---\n",
    "clf.train()\n",
    "for epoch in range(30):\n",
    "    total_loss = 0.0\n",
    "    for xb, yb in loader:\n",
    "        opt.zero_grad()\n",
    "        logits = clf(xb)\n",
    "        loss = loss_fn(logits, yb)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        total_loss += loss.item() * xb.size(0)\n",
    "\n",
    "    avg_loss = total_loss / len(loader.dataset)\n",
    "    if (epoch+1) % 5 == 0 or epoch == 0:\n",
    "        print(f\"epoch {epoch+1:02d} | loss {avg_loss:.4f}\")\n",
    "\n",
    "# --- Eval ---\n",
    "clf.eval()\n",
    "with torch.no_grad():\n",
    "    pred = clf(X_test_t).argmax(dim=1).cpu().numpy()\n",
    "    y_true = y_test_t.cpu().numpy()\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "print(\"Accuracy:\", accuracy_score(y_true, pred))\n",
    "print(classification_report(y_true, pred, digits=3, target_names=le.classes_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261dfe6c-7ca5-4dc5-8016-81615eded4bc",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "In this notebook, we explored how pretrained transformer models can be used\n",
    "for protein sequence analysis.\n",
    "\n",
    "We started with raw protein sequences from UniProt and constructed\n",
    "a labeled dataset based on Pfam protein families.\n",
    "\n",
    "We then:\n",
    "\n",
    "1. Used ESM-2 to convert each protein sequence into a fixed-length embedding.\n",
    "2. Trained a simple linear classifier on these embeddings.\n",
    "3. Evaluated the model against ground-truth family labels.\n",
    "\n",
    "---\n",
    "\n",
    "## What Did We Learn?\n",
    "\n",
    "- ESM-2 embeddings contain meaningful biological information.\n",
    "- Protein families are largely separable using a simple linear model.\n",
    "- No sequence alignment or handcrafted features were required.\n",
    "- We did not fine-tune ESM-2 — we used it purely as a feature extractor.\n",
    "\n",
    "This shows that pretrained protein language models\n",
    "can serve as powerful general-purpose representations for downstream tasks.\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "To further explore model performance, we can:\n",
    "\n",
    "- Compare different classifiers (Logistic Regression, SVM, Neural Networks).\n",
    "- Compare different ESM-2 model sizes.\n",
    "- Evaluate performance across more protein families.\n",
    "- Analyze confusion between biologically related families.\n",
    "\n",
    "---\n",
    "\n",
    "This notebook demonstrates a practical workflow:\n",
    "\n",
    "Raw Sequences → ESM-2 Embeddings → Linear Classifier → Family Prediction\n",
    "\n",
    "This pipeline can be extended to many other biological prediction tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b88c878-51f7-4d16-b342-641b8f28ed3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

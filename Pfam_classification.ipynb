{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cecf580c-d4bf-4a53-b379-e530938b958e",
   "metadata": {},
   "source": [
    "# Transformers for Biological Sequence Analysis\n",
    "\n",
    "## Understanding Protein Sequences, Families, and the Usage of ESM-2\n",
    "\n",
    "- Proteins are biological macromolecules composed of amino acids arranged in a linear sequence.\n",
    "- Each protein sequence can be represented as a string of characters, where each character corresponds to one of the 20 standard amino acids.\n",
    "\n",
    "For example: `MKTLLILTCLVAVALARPKA...` This linear sequence determines:\n",
    "\n",
    "- The three-dimensional structure of the protein\n",
    "- Its biochemical function\n",
    "- Its evolutionary relationships\n",
    "\n",
    "Understanding how sequence relates to structure and function is a central problem in molecular biology.\n",
    "\n",
    "### What Are Protein Families?\n",
    "Protein families group together proteins that:\n",
    "\n",
    "- Share evolutionary ancestry\n",
    "- Have similar structural domains\n",
    "- Perform similar biological functions\n",
    "\n",
    "Databases such as Pfam classify proteins into families based on conserved sequence motifs and Hidden Markov models (HMMs).\n",
    "\n",
    "For example:\n",
    "\n",
    "- Kinases form one family (enzymes that transfer phosphate groups)\n",
    "- ABC transporters form another family (membrane transport proteins)\n",
    "- Response regulators form another family (signaling proteins)\n",
    "\n",
    "Traditionally, identifying family membership required sequence alignment and HMM scoring.  \n",
    "Modern transformer models like **ESM-2** learn rich protein representations directly from sequences, enabling downstream classification tasks such as protein family prediction.\n",
    "\n",
    "## Now, let's get familiar with What ESM-2 Is?\n",
    "\n",
    "**ESM-2 (Evolutionary Scale Modeling 2)** is a transformer-based protein language model trained on millions of protein sequences.\n",
    "\n",
    "Instead of using alignments, ESM-2 learns directly from raw amino acid sequences using a self-supervised objective known as **masked language modeling**.\n",
    "\n",
    "Through this training process, the model learns to:\n",
    "\n",
    "* Capture long-range dependencies in sequences\n",
    "* Encode structural information\n",
    "* Represent functional similarity\n",
    "* Embed evolutionary relationships\n",
    "\n",
    "The output of ESM-2 is a dense numerical representation (**embedding**) of each protein sequence.\n",
    "\n",
    "## There are several ESM-2 checkpoints with differing model sizes. \n",
    "Larger models will generally have better accuracy, but   they require more GPU memory and will take much longer to train. The available ESM-2 checkpoints are:\n",
    "\n",
    "| Checkpoint name        | Num layers | Num parameters |\n",
    "|------------------------|-----------:|---------------:|\n",
    "| esm2_t48_15B_UR50D     | 48         | 15B            |\n",
    "| esm2_t36_3B_UR50D      | 36         | 3B             |\n",
    "| esm2_t33_650M_UR50D    | 33         | 650M           |\n",
    "| esm2_t30_150M_UR50D    | 30         | 150M           |\n",
    "| esm2_t12_35M_UR50D     | 12         | 35M            |\n",
    "| esm2_t6_8M_UR50D       | 6          | 8M             |\n",
    "\n",
    "Note that the larger checkpoints may be very difficult to train without a large cloud GPU like an A100 or H100, and the largest 15B parameter checkpoint will probably be impossible to train on any single GPU! Also, note that memory usage for attention during training will scale as O(batch_size * num_layers * seq_len^2), so larger models on long sequences will use quite a lot of memory! We will use the esm2_t30_150M_UR50D checkpoint for this notebook, which should train on any notebook or modern GPU.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0780640d-9d71-4118-95d3-132c6a7e2b4a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Import the Libraries we will use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "812f24a8-6bd9-4674-97e8-a4d858a64e3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import requests\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import sklearn\n",
    "import matplotlib\n",
    "import re  # needed for UniProt paging helper\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, Optional\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cdcebbe-6320-4651-84d1-c6291ecbe2e1",
   "metadata": {},
   "source": [
    "## Verify Installation + GPU Availability\n",
    "\n",
    "We verify that libraries import correctly and check whether CUDA is available.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3096203d-4fb2-45ba-9305-65f48473cf38",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CUDA available: True\n",
      "GPU: NVIDIA L4\n"
     ]
    }
   ],
   "source": [
    "# Configuration class to store experiment settings in one place\n",
    "@dataclass\n",
    "class Config:\n",
    "    cache_dir: str = \"./data/cache_uniprot\"                      # folder to store downloaded/cached UniProt data\n",
    "    seed: int = 42                                               # random seed for reproducibility\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\" # automatically use GPU if available, otherwise fallback to CPU\n",
    "\n",
    "cfg = Config()\n",
    "\n",
    "os.makedirs(cfg.cache_dir, exist_ok=True)\n",
    "np.random.seed(cfg.seed)\n",
    "torch.manual_seed(cfg.seed)\n",
    "\n",
    "print(\"\\nCUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11fbf7fc-7b04-4bf0-b82b-2ed2b8f4f80d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ---------- USER CONTROLS ----------\n",
    "TARGET_FAMILIES = 200          # <-- set desired number of families (e.g., 100, 200, 500)\n",
    "PER_CLASS = 150                # sequences per family (balanced)\n",
    "MIN_AFTER_CLEAN = 80           # drop families that become too small after cleaning\n",
    "SCAN_PFAM_MAX = 20000          # how far to scan PFxxxxx IDs (increase if you want more choices)\n",
    "\n",
    "REVIEWED_ONLY = True           # True = Swiss-Prot only (cleaner, smaller). False = bigger, noisier.\n",
    "LENGTH_MIN = 50\n",
    "LENGTH_MAX = 1200\n",
    "\n",
    "# Model choice\n",
    "ESM_MODEL_NAME = \"facebook/esm2_t30_150M_UR50D\"   # <-- your requested model\n",
    "MAX_LEN = 512                 # 512 is fast; 1024 is heavier\n",
    "BATCH_SIZE = 4                # t30_150M needs smaller batch on many GPUs (try 4, 2, or 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2e62aa-523a-4676-9ffb-c05cb232e69e",
   "metadata": {},
   "source": [
    "## Retrieving Protein Sequences from UniProt (Programmatic Access)\n",
    "\n",
    "To build our dataset, we need to retrieve protein sequences and metadata from UniProt using their REST API.\n",
    "\n",
    "We will:\n",
    "1. Build a query that filters sequences by Pfam family (xref:pfam-PFxxxxx)\n",
    "2. Download sequences in TSV format (accession, sequence, organism, etc.)\n",
    "3. Cache results locally to avoid repeated downloads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16413b08-42de-4870-b34c-5cc14e360f8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "UNIPROT_SEARCH_URL = \"https://rest.uniprot.org/uniprotkb/search\"\n",
    "\n",
    "def _next_link_from_headers(headers: Dict[str, str]) -> Optional[str]:\n",
    "    link = headers.get(\"Link\")\n",
    "    if not link:\n",
    "        return None\n",
    "    m = re.search(r'<([^>]+)>;\\s*rel=\"next\"', link)\n",
    "    return m.group(1) if m else None\n",
    "\n",
    "def fetch_uniprot_tsv(query: str, fields: str, size: int = 500, max_rows: int = 1000) -> pd.DataFrame:\n",
    "    params = {\"query\": query, \"format\": \"tsv\", \"fields\": fields, \"size\": size}\n",
    "    rows = []\n",
    "    url = UNIPROT_SEARCH_URL\n",
    "\n",
    "    header = None\n",
    "    while url and len(rows) < max_rows:\n",
    "        r = requests.get(url, params=params if url == UNIPROT_SEARCH_URL else None, timeout=60)\n",
    "        r.raise_for_status()\n",
    "\n",
    "        text = r.text.strip()\n",
    "        if not text:\n",
    "            break\n",
    "\n",
    "        lines = text.splitlines()\n",
    "        header = lines[0].split(\"\\t\")\n",
    "        for line in lines[1:]:\n",
    "            rows.append(line.split(\"\\t\"))\n",
    "            if len(rows) >= max_rows:\n",
    "                break\n",
    "\n",
    "        url = _next_link_from_headers(r.headers)\n",
    "        params = None\n",
    "        time.sleep(0.1)\n",
    "\n",
    "    if not rows:\n",
    "        return pd.DataFrame(columns=fields.split(\",\"))\n",
    "    return pd.DataFrame(rows, columns=header)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9a2cf1d-1c2d-4e97-95ae-53a3b16c2508",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def uniprot_count(query: str) -> int:\n",
    "    \"\"\"\n",
    "    Returns total number of UniProt entries matching a query\n",
    "    without downloading full records.\n",
    "    \"\"\"\n",
    "    r = requests.get(\n",
    "        UNIPROT_SEARCH_URL,\n",
    "        params={\"query\": query, \"format\": \"json\", \"size\": 0},\n",
    "        timeout=30\n",
    "    )\n",
    "    if r.status_code != 200:\n",
    "        return 0\n",
    "\n",
    "    return int(r.headers.get(\"x-total-results\", \"0\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d089fadf-bbf8-4c7e-995d-ef5980ea2d83",
   "metadata": {},
   "source": [
    "## Selecting Protein Families for the Benchmark\n",
    "\n",
    "In this step, we identify Pfam families that contain enough protein sequences to support a balanced classification dataset.\n",
    "\n",
    "We will:\n",
    "- scan Pfam IDs (PF00001 ... PFxxxxx)\n",
    "- count how many sequences exist in UniProt for each family\n",
    "- keep only families with at least `PER_CLASS` sequences\n",
    "- select the first `TARGET_FAMILIES`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0e01671-eeed-4211-9a40-d4ba7ca29acb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanned PF00001..PF00200 | candidates so far: 122\n",
      "Scanned PF00001..PF00400 | candidates so far: 213\n",
      "Scanned PF00001..PF00600 | candidates so far: 297\n",
      "Candidate families found: 300\n",
      "First 10: ['PF00001', 'PF00004', 'PF00005', 'PF00006', 'PF00008', 'PF00009', 'PF00010', 'PF00011', 'PF00012', 'PF00013']\n"
     ]
    }
   ],
   "source": [
    "def build_query_for_pfam(pfam_id: str) -> str:\n",
    "    reviewed = \" AND (reviewed:true)\" if REVIEWED_ONLY else \"\"\n",
    "    pfam_clause = f\"(xref:pfam-{pfam_id})\"\n",
    "    return (\n",
    "        f\"{pfam_clause}\"\n",
    "        f\" AND (fragment:false)\"\n",
    "        f\"{reviewed}\"\n",
    "        f\" AND (length:[{LENGTH_MIN} TO {LENGTH_MAX}])\"\n",
    "    )\n",
    "\n",
    "# Discover candidates with enough sequences\n",
    "selected = []\n",
    "for i in range(1, SCAN_PFAM_MAX + 1):\n",
    "    pf = f\"PF{i:05d}\"\n",
    "    c = uniprot_count(build_query_for_pfam(pf))\n",
    "    if c >= PER_CLASS:\n",
    "        selected.append(pf)\n",
    "\n",
    "    # buffer so we don't lose too many families during cleaning\n",
    "    if len(selected) >= int(TARGET_FAMILIES * 1.5):\n",
    "        break\n",
    "\n",
    "    if i % 200 == 0:\n",
    "        print(f\"Scanned PF00001..PF{i:05d} | candidates so far: {len(selected)}\")\n",
    "        time.sleep(0.05)\n",
    "\n",
    "print(\"Candidate families found:\", len(selected))\n",
    "print(\"First 10:\", selected[:10])\n",
    "\n",
    "assert len(selected) >= TARGET_FAMILIES, (\n",
    "    f\"Not enough families found. Found {len(selected)} candidates, need {TARGET_FAMILIES}. \"\n",
    "    f\"Try: set REVIEWED_ONLY=False, lower PER_CLASS, increase SCAN_PFAM_MAX.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ccede9-3e02-430d-8072-217c383dcc65",
   "metadata": {},
   "source": [
    "## Build Dataset (Part A) : Standardize UniProt Column Names\n",
    "\n",
    "UniProt can return different column headers depending on fields and API updates.\n",
    "We normalize column names into a consistent schema:\n",
    "\n",
    "- accession\n",
    "- sequence\n",
    "- protein_name\n",
    "- organism_name\n",
    "- length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54a90741-6c91-433e-bb5f-ee2a62286579",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _normalize_uniprot_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Map lowercase column names -> original column names (case-insensitive matching)\n",
    "    cmap = {c.lower(): c for c in df.columns}\n",
    "\n",
    "    # Try multiple possible names UniProt might use\n",
    "    acc = cmap.get(\"accession\") or cmap.get(\"entry\")\n",
    "    seq = cmap.get(\"sequence\")\n",
    "    prot = cmap.get(\"protein_name\") or cmap.get(\"protein names\")\n",
    "    org  = cmap.get(\"organism_name\") or cmap.get(\"organism\")\n",
    "    leng = cmap.get(\"length\")\n",
    "\n",
    "    # accession + sequence are mandatory; if missing, stop early with a clear error\n",
    "    if acc is None or seq is None:\n",
    "        raise ValueError(f\"Missing accession/sequence. Columns: {list(df.columns)}\")\n",
    "\n",
    "    # Rename to our standard schema\n",
    "    return df.rename(columns={\n",
    "        acc: \"accession\",\n",
    "        seq: \"sequence\",\n",
    "        (prot or \"protein_name\"): \"protein_name\",\n",
    "        (org  or \"organism_name\"): \"organism_name\",\n",
    "        (leng or \"length\"): \"length\",\n",
    "    }).copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69269d5e-c83a-4491-8639-35072b894bfb",
   "metadata": {},
   "source": [
    "## Build Dataset (Part B) : Fetch One Family (with Caching)\n",
    "\n",
    "For each Pfam family:\n",
    "- build UniProt query\n",
    "- fetch up to `PER_CLASS` sequences\n",
    "- cache the results so we don't re-download each run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58ebca37-ee2c-486c-b52c-e521728e41b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_family_df(label: str, pfam_id: str, n: int) -> pd.DataFrame:\n",
    "    # Cache path so we don't re-download the same data every time\n",
    "    cache_path = os.path.join(\n",
    "        cfg.cache_dir, f\"{label}_{pfam_id}_n{n}_rev{int(REVIEWED_ONLY)}.tsv\"\n",
    "    )\n",
    "\n",
    "    # If cached file exists, load it immediately\n",
    "    if os.path.exists(cache_path):\n",
    "        return pd.read_csv(cache_path, sep=\"\\t\")\n",
    "\n",
    "    # Build query and fetch from UniProt\n",
    "    query = build_query_for_pfam(pfam_id)\n",
    "    fields = \"accession,protein_name,organism_name,length,sequence\"\n",
    "    df = fetch_uniprot_tsv(query=query, fields=fields, size=500, max_rows=n)\n",
    "\n",
    "    # If UniProt returns nothing, return empty DataFrame\n",
    "    if df.empty:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Normalize columns and basic cleaning\n",
    "    df = _normalize_uniprot_columns(df)\n",
    "    df = df.dropna(subset=[\"sequence\"]).copy()\n",
    "\n",
    "    # Add label column (ground-truth family)\n",
    "    df[\"label\"] = label\n",
    "\n",
    "    # Save to cache\n",
    "    df.to_csv(cache_path, sep=\"\\t\", index=False)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d455b986-686a-4b57-86dc-537dd4a83737",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Build Dataset (Part C): Collect Many Families into One Dataset\n",
    "\n",
    "We now:\n",
    "- Select the first `TARGET_FAMILIES` families\n",
    "- Fetch `PER_CLASS` sequences per family\n",
    "- Concatenate into one dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae875feb-8838-49c9-a34c-91c257dba20f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PFAM_PF00001 PF00001 rows: 150\n",
      "PFAM_PF00004 PF00004 rows: 150\n",
      "PFAM_PF00005 PF00005 rows: 150\n",
      "PFAM_PF00006 PF00006 rows: 150\n",
      "PFAM_PF00008 PF00008 rows: 150\n",
      "PFAM_PF00009 PF00009 rows: 150\n",
      "PFAM_PF00010 PF00010 rows: 150\n",
      "PFAM_PF00011 PF00011 rows: 150\n",
      "PFAM_PF00012 PF00012 rows: 150\n",
      "PFAM_PF00013 PF00013 rows: 150\n",
      "PFAM_PF00014 PF00014 rows: 150\n",
      "PFAM_PF00016 PF00016 rows: 150\n",
      "PFAM_PF00017 PF00017 rows: 150\n",
      "PFAM_PF00018 PF00018 rows: 150\n",
      "PFAM_PF00019 PF00019 rows: 150\n",
      "PFAM_PF00022 PF00022 rows: 150\n",
      "PFAM_PF00023 PF00023 rows: 150\n",
      "PFAM_PF00025 PF00025 rows: 150\n",
      "PFAM_PF00026 PF00026 rows: 150\n",
      "PFAM_PF00027 PF00027 rows: 150\n",
      "PFAM_PF00028 PF00028 rows: 150\n",
      "PFAM_PF00032 PF00032 rows: 150\n",
      "PFAM_PF00033 PF00033 rows: 150\n",
      "PFAM_PF00034 PF00034 rows: 150\n",
      "PFAM_PF00035 PF00035 rows: 150\n",
      "PFAM_PF00038 PF00038 rows: 150\n",
      "PFAM_PF00041 PF00041 rows: 150\n",
      "PFAM_PF00042 PF00042 rows: 150\n",
      "PFAM_PF00043 PF00043 rows: 150\n",
      "PFAM_PF00044 PF00044 rows: 150\n",
      "PFAM_PF00046 PF00046 rows: 150\n",
      "PFAM_PF00047 PF00047 rows: 150\n",
      "PFAM_PF00048 PF00048 rows: 150\n",
      "PFAM_PF00049 PF00049 rows: 150\n",
      "PFAM_PF00056 PF00056 rows: 150\n",
      "PFAM_PF00059 PF00059 rows: 150\n",
      "PFAM_PF00061 PF00061 rows: 150\n",
      "PFAM_PF00067 PF00067 rows: 150\n",
      "PFAM_PF00068 PF00068 rows: 150\n",
      "PFAM_PF00069 PF00069 rows: 150\n",
      "PFAM_PF00071 PF00071 rows: 150\n",
      "PFAM_PF00072 PF00072 rows: 150\n",
      "PFAM_PF00074 PF00074 rows: 150\n",
      "PFAM_PF00075 PF00075 rows: 150\n",
      "PFAM_PF00076 PF00076 rows: 150\n",
      "PFAM_PF00079 PF00079 rows: 150\n",
      "PFAM_PF00080 PF00080 rows: 150\n",
      "PFAM_PF00081 PF00081 rows: 150\n",
      "PFAM_PF00082 PF00082 rows: 150\n",
      "PFAM_PF00083 PF00083 rows: 150\n",
      "PFAM_PF00084 PF00084 rows: 150\n",
      "PFAM_PF00085 PF00085 rows: 150\n",
      "PFAM_PF00089 PF00089 rows: 150\n",
      "PFAM_PF00091 PF00091 rows: 150\n",
      "PFAM_PF00096 PF00096 rows: 150\n",
      "PFAM_PF00097 PF00097 rows: 150\n",
      "PFAM_PF00098 PF00098 rows: 150\n",
      "PFAM_PF00101 PF00101 rows: 150\n",
      "PFAM_PF00103 PF00103 rows: 150\n",
      "PFAM_PF00104 PF00104 rows: 150\n",
      "PFAM_PF00105 PF00105 rows: 150\n",
      "PFAM_PF00106 PF00106 rows: 150\n",
      "PFAM_PF00107 PF00107 rows: 150\n",
      "PFAM_PF00108 PF00108 rows: 150\n",
      "PFAM_PF00111 PF00111 rows: 150\n",
      "PFAM_PF00112 PF00112 rows: 150\n",
      "PFAM_PF00113 PF00113 rows: 150\n",
      "PFAM_PF00115 PF00115 rows: 150\n",
      "PFAM_PF00116 PF00116 rows: 150\n",
      "PFAM_PF00117 PF00117 rows: 150\n",
      "PFAM_PF00118 PF00118 rows: 150\n",
      "PFAM_PF00119 PF00119 rows: 150\n",
      "PFAM_PF00120 PF00120 rows: 150\n",
      "PFAM_PF00121 PF00121 rows: 150\n",
      "PFAM_PF00122 PF00122 rows: 150\n",
      "PFAM_PF00124 PF00124 rows: 150\n",
      "PFAM_PF00125 PF00125 rows: 150\n",
      "PFAM_PF00126 PF00126 rows: 150\n",
      "PFAM_PF00128 PF00128 rows: 150\n",
      "PFAM_PF00130 PF00130 rows: 150\n",
      "PFAM_PF00132 PF00132 rows: 150\n",
      "PFAM_PF00133 PF00133 rows: 150\n",
      "PFAM_PF00134 PF00134 rows: 150\n",
      "PFAM_PF00137 PF00137 rows: 150\n",
      "PFAM_PF00139 PF00139 rows: 150\n",
      "PFAM_PF00141 PF00141 rows: 150\n",
      "PFAM_PF00142 PF00142 rows: 150\n",
      "PFAM_PF00146 PF00146 rows: 150\n",
      "PFAM_PF00148 PF00148 rows: 150\n",
      "PFAM_PF00149 PF00149 rows: 150\n",
      "PFAM_PF00152 PF00152 rows: 150\n",
      "PFAM_PF00153 PF00153 rows: 150\n",
      "PFAM_PF00154 PF00154 rows: 150\n",
      "PFAM_PF00155 PF00155 rows: 150\n",
      "PFAM_PF00156 PF00156 rows: 150\n",
      "PFAM_PF00158 PF00158 rows: 150\n",
      "PFAM_PF00160 PF00160 rows: 150\n",
      "PFAM_PF00162 PF00162 rows: 150\n",
      "PFAM_PF00163 PF00163 rows: 150\n",
      "PFAM_PF00164 PF00164 rows: 150\n",
      "PFAM_PF00166 PF00166 rows: 150\n",
      "PFAM_PF00168 PF00168 rows: 150\n",
      "PFAM_PF00169 PF00169 rows: 150\n",
      "PFAM_PF00170 PF00170 rows: 150\n",
      "PFAM_PF00171 PF00171 rows: 150\n",
      "PFAM_PF00172 PF00172 rows: 150\n",
      "PFAM_PF00173 PF00173 rows: 150\n",
      "PFAM_PF00175 PF00175 rows: 150\n",
      "PFAM_PF00176 PF00176 rows: 150\n",
      "PFAM_PF00177 PF00177 rows: 150\n",
      "PFAM_PF00179 PF00179 rows: 150\n",
      "PFAM_PF00180 PF00180 rows: 150\n",
      "PFAM_PF00181 PF00181 rows: 150\n",
      "PFAM_PF00183 PF00183 rows: 150\n",
      "PFAM_PF00185 PF00185 rows: 150\n",
      "PFAM_PF00188 PF00188 rows: 150\n",
      "PFAM_PF00189 PF00189 rows: 150\n",
      "PFAM_PF00190 PF00190 rows: 150\n",
      "PFAM_PF00195 PF00195 rows: 150\n",
      "PFAM_PF00196 PF00196 rows: 150\n",
      "PFAM_PF00199 PF00199 rows: 150\n",
      "PFAM_PF00200 PF00200 rows: 150\n",
      "PFAM_PF00201 PF00201 rows: 150\n",
      "PFAM_PF00202 PF00202 rows: 150\n",
      "PFAM_PF00203 PF00203 rows: 150\n",
      "PFAM_PF00205 PF00205 rows: 150\n",
      "PFAM_PF00206 PF00206 rows: 150\n",
      "PFAM_PF00210 PF00210 rows: 150\n",
      "PFAM_PF00213 PF00213 rows: 150\n",
      "PFAM_PF00215 PF00215 rows: 150\n",
      "PFAM_PF00216 PF00216 rows: 150\n",
      "PFAM_PF00218 PF00218 rows: 150\n",
      "PFAM_PF00221 PF00221 rows: 150\n",
      "PFAM_PF00223 PF00223 rows: 150\n",
      "PFAM_PF00225 PF00225 rows: 150\n",
      "PFAM_PF00226 PF00226 rows: 150\n",
      "PFAM_PF00227 PF00227 rows: 150\n",
      "PFAM_PF00230 PF00230 rows: 150\n",
      "PFAM_PF00231 PF00231 rows: 150\n",
      "PFAM_PF00232 PF00232 rows: 150\n",
      "PFAM_PF00234 PF00234 rows: 150\n",
      "PFAM_PF00235 PF00235 rows: 150\n",
      "PFAM_PF00237 PF00237 rows: 150\n",
      "PFAM_PF00238 PF00238 rows: 150\n",
      "PFAM_PF00240 PF00240 rows: 150\n",
      "PFAM_PF00248 PF00248 rows: 150\n",
      "PFAM_PF00249 PF00249 rows: 150\n",
      "PFAM_PF00250 PF00250 rows: 150\n",
      "PFAM_PF00252 PF00252 rows: 150\n",
      "PFAM_PF00253 PF00253 rows: 150\n",
      "PFAM_PF00254 PF00254 rows: 150\n",
      "PFAM_PF00258 PF00258 rows: 150\n",
      "PFAM_PF00265 PF00265 rows: 150\n",
      "PFAM_PF00266 PF00266 rows: 150\n",
      "PFAM_PF00270 PF00270 rows: 150\n",
      "PFAM_PF00271 PF00271 rows: 150\n",
      "PFAM_PF00275 PF00275 rows: 150\n",
      "PFAM_PF00276 PF00276 rows: 150\n",
      "PFAM_PF00281 PF00281 rows: 150\n",
      "PFAM_PF00282 PF00282 rows: 150\n",
      "PFAM_PF00288 PF00288 rows: 150\n",
      "PFAM_PF00290 PF00290 rows: 150\n",
      "PFAM_PF00291 PF00291 rows: 150\n",
      "PFAM_PF00293 PF00293 rows: 150\n",
      "PFAM_PF00294 PF00294 rows: 150\n",
      "PFAM_PF00295 PF00295 rows: 150\n",
      "PFAM_PF00296 PF00296 rows: 150\n",
      "PFAM_PF00297 PF00297 rows: 150\n",
      "PFAM_PF00298 PF00298 rows: 150\n",
      "PFAM_PF00300 PF00300 rows: 150\n",
      "PFAM_PF00303 PF00303 rows: 150\n",
      "PFAM_PF00306 PF00306 rows: 150\n",
      "PFAM_PF00307 PF00307 rows: 150\n",
      "PFAM_PF00308 PF00308 rows: 150\n",
      "PFAM_PF00311 PF00311 rows: 150\n",
      "PFAM_PF00312 PF00312 rows: 150\n",
      "PFAM_PF00313 PF00313 rows: 150\n",
      "PFAM_PF00316 PF00316 rows: 150\n",
      "PFAM_PF00318 PF00318 rows: 150\n",
      "PFAM_PF00319 PF00319 rows: 150\n",
      "PFAM_PF00320 PF00320 rows: 150\n",
      "PFAM_PF00326 PF00326 rows: 150\n",
      "PFAM_PF00327 PF00327 rows: 150\n",
      "PFAM_PF00329 PF00329 rows: 150\n",
      "PFAM_PF00330 PF00330 rows: 150\n",
      "PFAM_PF00333 PF00333 rows: 150\n",
      "PFAM_PF00334 PF00334 rows: 150\n",
      "PFAM_PF00335 PF00335 rows: 150\n",
      "PFAM_PF00338 PF00338 rows: 150\n",
      "PFAM_PF00342 PF00342 rows: 150\n",
      "PFAM_PF00344 PF00344 rows: 150\n",
      "PFAM_PF00346 PF00346 rows: 150\n",
      "PFAM_PF00347 PF00347 rows: 150\n",
      "PFAM_PF00348 PF00348 rows: 150\n",
      "PFAM_PF00355 PF00355 rows: 150\n",
      "PFAM_PF00356 PF00356 rows: 150\n",
      "PFAM_PF00361 PF00361 rows: 150\n",
      "PFAM_PF00364 PF00364 rows: 150\n",
      "PFAM_PF00365 PF00365 rows: 150\n",
      "PFAM_PF00366 PF00366 rows: 150\n",
      "\n",
      "Raw rows: 30000 Raw classes: 200\n"
     ]
    }
   ],
   "source": [
    "pfams = selected[:TARGET_FAMILIES]\n",
    "FAMILIES = {f\"PFAM_{pf}\": pf for pf in pfams}\n",
    "\n",
    "dfs = []\n",
    "for label, pf in FAMILIES.items():\n",
    "    df_i = get_family_df(label, pf, PER_CLASS)\n",
    "    if not df_i.empty:\n",
    "        dfs.append(df_i)\n",
    "    print(label, pf, \"rows:\", len(df_i))\n",
    "\n",
    "data = pd.concat(dfs, ignore_index=True)\n",
    "print(\"\\nRaw rows:\", len(data), \"Raw classes:\", data[\"label\"].nunique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904fe13a-e47c-4101-8c78-fd87377def10",
   "metadata": {},
   "source": [
    "## Build Dataset (Part D): Remove Duplicate Sequences\n",
    "\n",
    "The same protein sequence can appear multiple times across UniProt entries.\n",
    "We remove exact duplicate sequences to prevent data leakage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f143996-7489-4ade-ae1b-5cb96148b9cb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After de-dup sequences: 27065\n"
     ]
    }
   ],
   "source": [
    "data = data.drop_duplicates(subset=[\"sequence\"]).reset_index(drop=True)\n",
    "print(\"After de-dup sequences:\", len(data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aaf26bb-1953-444a-8145-7fd92d537f69",
   "metadata": {},
   "source": [
    "## Build Dataset (Part E): Remove Multi-Label Accessions\n",
    "\n",
    "A protein can sometimes map to multiple families (rare but possible).\n",
    "To keep a clean classification setup, we remove accessions that appear under multiple labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "005eaebe-906d-4585-ac75-9c3ea58be3f2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After removing multi-label accessions: 27065\n"
     ]
    }
   ],
   "source": [
    "acc_n = data.groupby(\"accession\")[\"label\"].nunique()\n",
    "data = data[data[\"accession\"].isin(acc_n[acc_n == 1].index)].reset_index(drop=True)\n",
    "print(\"After removing multi-label accessions:\", len(data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7c5a40-8954-4e51-8b47-3812302e31b2",
   "metadata": {},
   "source": [
    "## Build Dataset (Part F): Drop Families That Became Too Small\n",
    "\n",
    "After cleaning, some families might lose too many sequences.\n",
    "We drop families with < `MIN_AFTER_CLEAN` sequences to keep classes meaningful.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4cddc31b-3ef5-4c67-a50d-10e4194bf9fc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final rows: 27026\n",
      "Final classes: 197\n",
      "Per-class min/median/max: 82 141 150\n"
     ]
    }
   ],
   "source": [
    "vc = data[\"label\"].value_counts()\n",
    "keep = vc[vc >= MIN_AFTER_CLEAN].index\n",
    "data = data[data[\"label\"].isin(keep)].reset_index(drop=True)\n",
    "\n",
    "print(\"Final rows:\", len(data))\n",
    "print(\"Final classes:\", data[\"label\"].nunique())\n",
    "print(\"Per-class min/median/max:\",\n",
    "      data[\"label\"].value_counts().min(),\n",
    "      int(data[\"label\"].value_counts().median()),\n",
    "      data[\"label\"].value_counts().max())\n",
    "\n",
    "assert data[\"label\"].nunique() >= int(0.9 * TARGET_FAMILIES), (\n",
    "    \"Too many families dropped after cleaning. Try lowering MIN_AFTER_CLEAN or PER_CLASS, \"\n",
    "    \"or set REVIEWED_ONLY=False to get more sequences.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ca0a40-8379-4e91-950d-9f33d4feb0c8",
   "metadata": {},
   "source": [
    "## Generating Protein Embeddings Using ESM-2\n",
    "\n",
    "We now embed each protein sequence using ESM-2.\n",
    "\n",
    "Steps:\n",
    "- Tokenize sequences\n",
    "- Run through ESM-2 transformer\n",
    "- Mean-pool token embeddings (masked) to get one vector per protein\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3982be0a-0c2e-4c1e-b69f-511a17f514dd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding with: facebook/esm2_t30_150M_UR50D\n",
      "Device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping import of cpp extensions due to incompatible torch version 2.9.1+cu128 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info\n",
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t30_150M_UR50D and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Embedding: 100%|██████████| 6757/6757 [10:25<00:00, 10.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (27026, 640)\n",
      "Classes: 197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "assert \"data\" in globals() and not data.empty\n",
    "\n",
    "DEVICE = cfg.device\n",
    "MODEL_NAME = ESM_MODEL_NAME\n",
    "\n",
    "print(\"Embedding with:\", MODEL_NAME)\n",
    "print(\"Device:\", DEVICE)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME).to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "@torch.no_grad()\n",
    "def embed_batch(seqs):\n",
    "    seqs = [str(s)[:MAX_LEN] for s in seqs]\n",
    "    toks = tokenizer(seqs, return_tensors=\"pt\", padding=True, truncation=True, max_length=MAX_LEN)\n",
    "    toks = {k: v.to(DEVICE) for k, v in toks.items()}\n",
    "    out = model(**toks).last_hidden_state          # [B, T, H]\n",
    "    mask = toks[\"attention_mask\"].unsqueeze(-1)    # [B, T, 1]\n",
    "    pooled = (out * mask).sum(dim=1) / mask.sum(dim=1).clamp(min=1)\n",
    "    return pooled.float().cpu().numpy()\n",
    "\n",
    "def embed_dataset(df, batch_size=BATCH_SIZE):\n",
    "    seqs = df[\"sequence\"].astype(str).tolist()\n",
    "    embs = []\n",
    "    for i in tqdm(range(0, len(seqs), batch_size), desc=\"Embedding\"):\n",
    "        embs.append(embed_batch(seqs[i:i+batch_size]))\n",
    "    return np.vstack(embs)\n",
    "\n",
    "X = embed_dataset(data)\n",
    "y = data[\"label\"].values\n",
    "\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"Classes:\", len(np.unique(y)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53743c1-8d1f-451d-a992-dd44045bf698",
   "metadata": {},
   "source": [
    "## Training a Linear Classifier on ESM-2 Embeddings\n",
    "\n",
    "We:\n",
    "- encode string labels into integer classes\n",
    "- stratified train/test split\n",
    "- standardize using train statistics only (avoid leakage)\n",
    "- train a linear classifier (PyTorch) on GPU\n",
    "- evaluate accuracy and classification report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6902f39-443e-490b-bd4b-d93e3d9e75d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 01 | loss 0.6798\n",
      "epoch 05 | loss 0.0399\n",
      "epoch 10 | loss 0.0232\n",
      "epoch 15 | loss 0.0217\n",
      "epoch 20 | loss 0.0179\n",
      "epoch 25 | loss 0.0162\n",
      "epoch 30 | loss 0.0155\n",
      "Accuracy: 0.9758768684327365\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "PFAM_PF00001      1.000     1.000     1.000        38\n",
      "PFAM_PF00004      0.921     0.946     0.933        37\n",
      "PFAM_PF00005      1.000     0.973     0.986        37\n",
      "PFAM_PF00006      0.714     0.946     0.814        37\n",
      "PFAM_PF00008      0.921     0.921     0.921        38\n",
      "PFAM_PF00009      1.000     0.970     0.985        33\n",
      "PFAM_PF00010      0.925     0.974     0.949        38\n",
      "PFAM_PF00011      1.000     0.972     0.986        36\n",
      "PFAM_PF00012      1.000     1.000     1.000        34\n",
      "PFAM_PF00013      1.000     0.943     0.971        35\n",
      "PFAM_PF00014      1.000     1.000     1.000        37\n",
      "PFAM_PF00016      1.000     1.000     1.000        36\n",
      "PFAM_PF00017      0.854     0.946     0.897        37\n",
      "PFAM_PF00018      0.818     0.692     0.750        26\n",
      "PFAM_PF00019      0.974     1.000     0.987        37\n",
      "PFAM_PF00022      1.000     1.000     1.000        26\n",
      "PFAM_PF00023      0.949     0.974     0.961        38\n",
      "PFAM_PF00025      0.941     0.970     0.955        33\n",
      "PFAM_PF00026      1.000     1.000     1.000        36\n",
      "PFAM_PF00027      1.000     0.973     0.986        37\n",
      "PFAM_PF00028      1.000     1.000     1.000        38\n",
      "PFAM_PF00032      1.000     0.947     0.973        38\n",
      "PFAM_PF00034      1.000     1.000     1.000        34\n",
      "PFAM_PF00035      1.000     0.912     0.954        34\n",
      "PFAM_PF00038      1.000     1.000     1.000        38\n",
      "PFAM_PF00041      0.889     0.865     0.877        37\n",
      "PFAM_PF00042      1.000     0.943     0.971        35\n",
      "PFAM_PF00043      1.000     0.974     0.987        38\n",
      "PFAM_PF00044      1.000     1.000     1.000        34\n",
      "PFAM_PF00046      1.000     0.974     0.987        38\n",
      "PFAM_PF00047      0.943     0.943     0.943        35\n",
      "PFAM_PF00048      1.000     1.000     1.000        37\n",
      "PFAM_PF00049      1.000     1.000     1.000        36\n",
      "PFAM_PF00056      1.000     1.000     1.000        36\n",
      "PFAM_PF00059      0.944     0.971     0.958        35\n",
      "PFAM_PF00061      1.000     1.000     1.000        35\n",
      "PFAM_PF00067      1.000     0.974     0.987        38\n",
      "PFAM_PF00068      1.000     1.000     1.000        37\n",
      "PFAM_PF00069      0.935     0.784     0.853        37\n",
      "PFAM_PF00071      1.000     0.944     0.971        36\n",
      "PFAM_PF00072      0.882     0.811     0.845        37\n",
      "PFAM_PF00074      1.000     1.000     1.000        37\n",
      "PFAM_PF00075      1.000     1.000     1.000        32\n",
      "PFAM_PF00076      0.791     0.919     0.850        37\n",
      "PFAM_PF00079      1.000     1.000     1.000        38\n",
      "PFAM_PF00080      1.000     1.000     1.000        35\n",
      "PFAM_PF00081      1.000     1.000     1.000        34\n",
      "PFAM_PF00082      0.974     1.000     0.987        37\n",
      "PFAM_PF00083      1.000     1.000     1.000        38\n",
      "PFAM_PF00084      0.929     0.867     0.897        30\n",
      "PFAM_PF00085      0.925     0.974     0.949        38\n",
      "PFAM_PF00089      1.000     0.903     0.949        31\n",
      "PFAM_PF00091      1.000     1.000     1.000        27\n",
      "PFAM_PF00096      1.000     1.000     1.000        38\n",
      "PFAM_PF00097      0.875     0.946     0.909        37\n",
      "PFAM_PF00098      0.969     0.838     0.899        37\n",
      "PFAM_PF00101      1.000     1.000     1.000        37\n",
      "PFAM_PF00103      1.000     1.000     1.000        37\n",
      "PFAM_PF00104      1.000     1.000     1.000        37\n",
      "PFAM_PF00106      1.000     1.000     1.000        37\n",
      "PFAM_PF00107      1.000     1.000     1.000        38\n",
      "PFAM_PF00108      1.000     1.000     1.000        36\n",
      "PFAM_PF00111      0.972     0.972     0.972        36\n",
      "PFAM_PF00112      0.974     1.000     0.987        37\n",
      "PFAM_PF00113      1.000     1.000     1.000        34\n",
      "PFAM_PF00115      1.000     1.000     1.000        36\n",
      "PFAM_PF00116      1.000     1.000     1.000        34\n",
      "PFAM_PF00117      0.946     1.000     0.972        35\n",
      "PFAM_PF00118      1.000     1.000     1.000        35\n",
      "PFAM_PF00119      1.000     1.000     1.000        36\n",
      "PFAM_PF00120      1.000     0.971     0.986        35\n",
      "PFAM_PF00121      0.971     1.000     0.986        34\n",
      "PFAM_PF00122      1.000     1.000     1.000        37\n",
      "PFAM_PF00124      0.935     1.000     0.967        29\n",
      "PFAM_PF00125      0.966     1.000     0.982        28\n",
      "PFAM_PF00126      1.000     0.970     0.985        33\n",
      "PFAM_PF00128      1.000     1.000     1.000        37\n",
      "PFAM_PF00130      0.811     0.968     0.882        31\n",
      "PFAM_PF00132      1.000     1.000     1.000        35\n",
      "PFAM_PF00133      1.000     1.000     1.000        36\n",
      "PFAM_PF00134      1.000     1.000     1.000        37\n",
      "PFAM_PF00137      1.000     1.000     1.000        30\n",
      "PFAM_PF00139      1.000     1.000     1.000        38\n",
      "PFAM_PF00141      1.000     1.000     1.000        37\n",
      "PFAM_PF00142      1.000     1.000     1.000        36\n",
      "PFAM_PF00146      1.000     1.000     1.000        36\n",
      "PFAM_PF00148      1.000     1.000     1.000        38\n",
      "PFAM_PF00149      1.000     1.000     1.000        34\n",
      "PFAM_PF00152      1.000     1.000     1.000        35\n",
      "PFAM_PF00153      1.000     1.000     1.000        38\n",
      "PFAM_PF00154      1.000     1.000     1.000        28\n",
      "PFAM_PF00155      1.000     1.000     1.000        37\n",
      "PFAM_PF00156      0.967     1.000     0.983        29\n",
      "PFAM_PF00158      0.917     1.000     0.957        33\n",
      "PFAM_PF00160      1.000     1.000     1.000        34\n",
      "PFAM_PF00162      1.000     0.971     0.985        34\n",
      "PFAM_PF00163      1.000     1.000     1.000        34\n",
      "PFAM_PF00164      1.000     1.000     1.000        31\n",
      "PFAM_PF00166      1.000     1.000     1.000        29\n",
      "PFAM_PF00168      0.943     0.971     0.957        34\n",
      "PFAM_PF00169      0.786     0.786     0.786        28\n",
      "PFAM_PF00170      1.000     1.000     1.000        37\n",
      "PFAM_PF00171      1.000     1.000     1.000        37\n",
      "PFAM_PF00172      1.000     0.892     0.943        37\n",
      "PFAM_PF00173      1.000     0.973     0.986        37\n",
      "PFAM_PF00175      0.667     0.640     0.653        25\n",
      "PFAM_PF00176      0.846     0.971     0.904        34\n",
      "PFAM_PF00177      0.935     1.000     0.967        29\n",
      "PFAM_PF00179      1.000     1.000     1.000        31\n",
      "PFAM_PF00180      0.974     1.000     0.987        37\n",
      "PFAM_PF00181      1.000     1.000     1.000        26\n",
      "PFAM_PF00183      1.000     1.000     1.000        36\n",
      "PFAM_PF00185      1.000     1.000     1.000        33\n",
      "PFAM_PF00188      1.000     0.973     0.986        37\n",
      "PFAM_PF00189      0.970     0.970     0.970        33\n",
      "PFAM_PF00190      1.000     1.000     1.000        37\n",
      "PFAM_PF00195      1.000     1.000     1.000        36\n",
      "PFAM_PF00196      0.862     1.000     0.926        25\n",
      "PFAM_PF00199      0.971     1.000     0.986        34\n",
      "PFAM_PF00200      0.974     1.000     0.987        38\n",
      "PFAM_PF00201      1.000     1.000     1.000        38\n",
      "PFAM_PF00202      0.971     1.000     0.985        33\n",
      "PFAM_PF00203      1.000     1.000     1.000        31\n",
      "PFAM_PF00205      1.000     1.000     1.000        34\n",
      "PFAM_PF00206      1.000     1.000     1.000        34\n",
      "PFAM_PF00210      1.000     1.000     1.000        28\n",
      "PFAM_PF00213      1.000     1.000     1.000        34\n",
      "PFAM_PF00215      1.000     0.970     0.985        33\n",
      "PFAM_PF00216      1.000     1.000     1.000        29\n",
      "PFAM_PF00218      1.000     0.970     0.985        33\n",
      "PFAM_PF00221      1.000     0.973     0.986        37\n",
      "PFAM_PF00223      1.000     1.000     1.000        34\n",
      "PFAM_PF00225      0.974     1.000     0.987        38\n",
      "PFAM_PF00226      1.000     0.944     0.971        36\n",
      "PFAM_PF00227      1.000     1.000     1.000        37\n",
      "PFAM_PF00230      1.000     0.947     0.973        38\n",
      "PFAM_PF00231      1.000     1.000     1.000        34\n",
      "PFAM_PF00232      1.000     1.000     1.000        37\n",
      "PFAM_PF00234      1.000     1.000     1.000        37\n",
      "PFAM_PF00235      1.000     1.000     1.000        32\n",
      "PFAM_PF00237      1.000     1.000     1.000        33\n",
      "PFAM_PF00238      1.000     1.000     1.000        32\n",
      "PFAM_PF00240      0.970     0.941     0.955        34\n",
      "PFAM_PF00248      0.974     1.000     0.987        37\n",
      "PFAM_PF00249      0.944     0.944     0.944        36\n",
      "PFAM_PF00250      0.974     1.000     0.987        38\n",
      "PFAM_PF00252      1.000     0.938     0.968        32\n",
      "PFAM_PF00253      0.968     0.938     0.952        32\n",
      "PFAM_PF00254      1.000     1.000     1.000        37\n",
      "PFAM_PF00258      0.682     0.682     0.682        22\n",
      "PFAM_PF00265      1.000     1.000     1.000        32\n",
      "PFAM_PF00266      1.000     1.000     1.000        36\n",
      "PFAM_PF00270      0.923     1.000     0.960        36\n",
      "PFAM_PF00275      1.000     1.000     1.000        35\n",
      "PFAM_PF00276      1.000     1.000     1.000        33\n",
      "PFAM_PF00281      1.000     0.939     0.969        33\n",
      "PFAM_PF00282      1.000     1.000     1.000        37\n",
      "PFAM_PF00288      1.000     1.000     1.000        36\n",
      "PFAM_PF00290      1.000     1.000     1.000        35\n",
      "PFAM_PF00291      1.000     1.000     1.000        35\n",
      "PFAM_PF00293      1.000     1.000     1.000        35\n",
      "PFAM_PF00294      1.000     1.000     1.000        33\n",
      "PFAM_PF00295      1.000     1.000     1.000        34\n",
      "PFAM_PF00296      1.000     1.000     1.000        33\n",
      "PFAM_PF00297      1.000     1.000     1.000        35\n",
      "PFAM_PF00298      1.000     1.000     1.000        30\n",
      "PFAM_PF00300      1.000     0.971     0.986        35\n",
      "PFAM_PF00303      0.971     0.971     0.971        35\n",
      "PFAM_PF00306      0.778     0.333     0.467        21\n",
      "PFAM_PF00307      0.921     1.000     0.959        35\n",
      "PFAM_PF00308      1.000     0.969     0.984        32\n",
      "PFAM_PF00311      0.970     1.000     0.985        32\n",
      "PFAM_PF00312      1.000     1.000     1.000        32\n",
      "PFAM_PF00313      1.000     1.000     1.000        23\n",
      "PFAM_PF00316      1.000     1.000     1.000        32\n",
      "PFAM_PF00318      1.000     1.000     1.000        34\n",
      "PFAM_PF00319      0.914     0.865     0.889        37\n",
      "PFAM_PF00320      0.846     0.917     0.880        36\n",
      "PFAM_PF00326      1.000     1.000     1.000        36\n",
      "PFAM_PF00327      1.000     1.000     1.000        33\n",
      "PFAM_PF00329      0.939     1.000     0.969        31\n",
      "PFAM_PF00330      1.000     1.000     1.000        36\n",
      "PFAM_PF00333      1.000     1.000     1.000        34\n",
      "PFAM_PF00334      0.966     0.875     0.918        32\n",
      "PFAM_PF00335      1.000     1.000     1.000        36\n",
      "PFAM_PF00338      0.968     1.000     0.984        30\n",
      "PFAM_PF00342      1.000     1.000     1.000        36\n",
      "PFAM_PF00344      1.000     1.000     1.000        32\n",
      "PFAM_PF00346      1.000     1.000     1.000        20\n",
      "PFAM_PF00347      1.000     1.000     1.000        33\n",
      "PFAM_PF00348      1.000     1.000     1.000        37\n",
      "PFAM_PF00355      0.974     1.000     0.987        37\n",
      "PFAM_PF00356      1.000     1.000     1.000        25\n",
      "PFAM_PF00361      1.000     1.000     1.000        35\n",
      "PFAM_PF00364      1.000     1.000     1.000        35\n",
      "PFAM_PF00365      1.000     1.000     1.000        32\n",
      "PFAM_PF00366      1.000     0.968     0.984        31\n",
      "\n",
      "    accuracy                          0.976      6757\n",
      "   macro avg      0.975     0.973     0.973      6757\n",
      "weighted avg      0.977     0.976     0.976      6757\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# --- Encode labels ---\n",
    "le = LabelEncoder()\n",
    "y_int = le.fit_transform(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_int, test_size=0.25, random_state=42, stratify=y_int\n",
    ")\n",
    "\n",
    "# --- Standardize using train stats only (VERY IMPORTANT) ---\n",
    "mu = X_train.mean(axis=0, keepdims=True)\n",
    "std = X_train.std(axis=0, keepdims=True) + 1e-6\n",
    "X_train_s = (X_train - mu) / std\n",
    "X_test_s  = (X_test  - mu) / std\n",
    "\n",
    "device = cfg.device\n",
    "num_classes = len(le.classes_)\n",
    "in_dim = X_train_s.shape[1]\n",
    "\n",
    "# --- Tensors on GPU ---\n",
    "X_train_t = torch.tensor(X_train_s, dtype=torch.float32, device=device)\n",
    "y_train_t = torch.tensor(y_train,   dtype=torch.long,   device=device)\n",
    "X_test_t  = torch.tensor(X_test_s,  dtype=torch.float32, device=device)\n",
    "y_test_t  = torch.tensor(y_test,    dtype=torch.long,   device=device)\n",
    "\n",
    "# --- DataLoader (mini-batches) ---\n",
    "ds = TensorDataset(X_train_t, y_train_t)\n",
    "loader = DataLoader(ds, batch_size=256, shuffle=True)\n",
    "\n",
    "# --- Linear classifier ---\n",
    "clf = torch.nn.Linear(in_dim, num_classes).to(device)\n",
    "\n",
    "# Try SGD for linear models (often better behaved than Adam here)\n",
    "opt = torch.optim.SGD(clf.parameters(), lr=0.2, momentum=0.9, weight_decay=1e-4)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# --- Train ---\n",
    "clf.train()\n",
    "for epoch in range(30):\n",
    "    total_loss = 0.0\n",
    "    for xb, yb in loader:\n",
    "        opt.zero_grad()\n",
    "        logits = clf(xb)\n",
    "        loss = loss_fn(logits, yb)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        total_loss += loss.item() * xb.size(0)\n",
    "\n",
    "    avg_loss = total_loss / len(loader.dataset)\n",
    "    if (epoch+1) % 5 == 0 or epoch == 0:\n",
    "        print(f\"epoch {epoch+1:02d} | loss {avg_loss:.4f}\")\n",
    "\n",
    "# --- Eval ---\n",
    "clf.eval()\n",
    "with torch.no_grad():\n",
    "    pred = clf(X_test_t).argmax(dim=1).cpu().numpy()\n",
    "    y_true = y_test_t.cpu().numpy()\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "print(\"Accuracy:\", accuracy_score(y_true, pred))\n",
    "print(classification_report(y_true, pred, digits=3, target_names=le.classes_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b5a6a5-859d-4e6c-bbac-7592c81b9e7f",
   "metadata": {},
   "source": [
    "## Interpreting the Final Results\n",
    "\n",
    "The model achieved:\n",
    "\n",
    "- **Accuracy: 97.6%**\n",
    "- **Macro F1-score: 0.973**\n",
    "- **Weighted F1-score: 0.976**\n",
    "- Evaluation performed on 6,757 test proteins.\n",
    "\n",
    "\n",
    "### What Does This Mean?\n",
    "\n",
    "**Accuracy (97.6%)**\n",
    "→ 97.6% of test protein sequences were assigned to the correct Pfam family.\n",
    "\n",
    "\n",
    "### Macro Average\n",
    "\n",
    "The **macro average** computes metrics equally across all families.\n",
    "\n",
    "This tells us:\n",
    "- Performance is consistently high across families.\n",
    "- The model is not biased toward larger families.\n",
    "\n",
    "Macro F1 ≈ 0.973 indicates strong performance even on smaller classes.\n",
    "\n",
    "\n",
    "### Weighted Average\n",
    "\n",
    "The **weighted average** accounts for how many sequences each family has.\n",
    "\n",
    "Since weighted F1 ≈ accuracy, this means:\n",
    "- Performance is balanced.\n",
    "- No severe class imbalance issues remain.\n",
    "\n",
    "\n",
    "## What Is Actually Happening?\n",
    "\n",
    "1. Raw protein sequences were converted into embeddings using ESM-2.\n",
    "2. These embeddings represent proteins in a high-dimensional space.\n",
    "3. Proteins belonging to the same family cluster together.\n",
    "4. A linear classifier learned boundaries separating these clusters.\n",
    "\n",
    "High accuracy indicates that:\n",
    "\n",
    "- ESM-2 embeddings encode functional similarity.\n",
    "- Protein families are largely linearly separable in embedding space.\n",
    "- The transformer model captured meaningful biological structure.\n",
    "\n",
    "\n",
    "## Final Insight\n",
    "\n",
    "We did not fine-tune ESM-2.\n",
    "\n",
    "The pretrained model alone produced embeddings strong enough\n",
    "to enable near-perfect family classification using a simple linear layer.\n",
    "\n",
    "This demonstrates the power of large pretrained protein language models.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
